{"cells":[{"cell_type":"markdown","metadata":{"id":"0tR7yrpGkm2N"},"source":["#Imports \n"],"id":"0tR7yrpGkm2N"},{"cell_type":"code","execution_count":45,"metadata":{"id":"50870812","executionInfo":{"status":"ok","timestamp":1650691640552,"user_tz":-270,"elapsed":348,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt \n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import LabelBinarizer\n","import h5py\n","from scipy import ndimage\n","import scipy\n","from PIL import Image\n","import copy\n","import time \n","import math\n","import pickle\n","%matplotlib inline\n"],"id":"50870812"},{"cell_type":"markdown","source":["#Implementaions"],"metadata":{"id":"tnDcg4E71hRi"},"id":"tnDcg4E71hRi"},{"cell_type":"markdown","metadata":{"id":"uUPnQGX0lDr1"},"source":["##Activation functions"],"id":"uUPnQGX0lDr1"},{"cell_type":"markdown","source":["###Forward pass"],"metadata":{"id":"ayvAYp8YSe5g"},"id":"ayvAYp8YSe5g"},{"cell_type":"code","execution_count":3,"metadata":{"id":"b2f02e44","executionInfo":{"status":"ok","timestamp":1650690405644,"user_tz":-270,"elapsed":7,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["# Needed Functions \n","\n","def sigmoid(Z):\n","    \"\"\"\n","    Implements the sigmoid activation in numpy\n","    \n","    Arguments:\n","    Z -- numpy array of any shape\n","    \n","    Returns:\n","    A -- output of sigmoid(z), same shape as Z\n","    cache -- returns Z as well, useful during backpropagation\n","    \"\"\"\n","    \n","    A = 1/(1+np.exp(-Z))\n","    cache = Z\n","    \n","    return A, cache\n","\n","def relu(Z):\n","    \"\"\"\n","    Implement the RELU function.\n","\n","    Arguments:\n","    Z -- Output of the linear layer, of any shape\n","\n","    Returns:\n","    A -- Post-activation parameter, of the same shape as Z\n","    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    A = np.maximum(0,Z)\n","    \n","    assert(A.shape == Z.shape)\n","    \n","    cache = Z \n","    return A, cache\n","\n","\n","def softmax(Z):\n","  \"\"\"\n","  Implement Softmax activation (Multi-class classification)\n","\n","  Arguments:\n","  Z -- output of the linear layer\n","\n","  Returns:\n","  A -- Post_activation parameter, the same shape as Z\n","  cache -- Z to use in backprop\n","  \"\"\"\n","\n","  t = np.exp(Z)\n","  A = t / np.sum(t, axis=0)\n","  \n","  assert(A.shape == Z.shape)\n","\n","  cache = Z\n","  return A, cache"],"id":"b2f02e44"},{"cell_type":"markdown","source":["###Backward pass"],"metadata":{"id":"vtiQfFk5Sm-p"},"id":"vtiQfFk5Sm-p"},{"cell_type":"code","source":["\n","def relu_backward(dA, cache):\n","    \"\"\"\n","    Implement the backward propagation for a single RELU unit.\n","\n","    Arguments:\n","    dA -- post-activation gradient, of any shape\n","    cache -- 'Z' where we store for computing backward propagation efficiently\n","\n","    Returns:\n","    dZ -- Gradient of the cost with respect to Z\n","    \"\"\"\n","    \n","    Z = cache\n","    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n","    \n","    # When z <= 0, you should set dz to 0 as well. \n","    dZ[Z <= 0] = 0\n","    \n","    assert (dZ.shape == Z.shape)\n","    \n","    return dZ\n","\n","def sigmoid_backward(dA, cache):\n","    \"\"\"\n","    Implement the backward propagation for a single SIGMOID unit.\n","\n","    Arguments:\n","    dA -- post-activation gradient, of any shape\n","    cache -- 'Z' where we store for computing backward propagation efficiently\n","\n","    Returns:\n","    dZ -- Gradient of the cost with respect to Z\n","    \"\"\"\n","    \n","    Z = cache\n","    \n","    s = 1/(1+np.exp(-Z))\n","    dZ = dA * s * (1-s)\n","    \n","    assert (dZ.shape == Z.shape)\n","    \n","    return dZ\n"],"metadata":{"id":"T8KKXYw8SpE2","executionInfo":{"status":"ok","timestamp":1650690405644,"user_tz":-270,"elapsed":6,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"id":"T8KKXYw8SpE2","execution_count":4,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Kohh8g5XlN5W"},"source":["##Initialization"],"id":"Kohh8g5XlN5W"},{"cell_type":"markdown","source":["###Weights"],"metadata":{"id":"2iQ9O81in8u0"},"id":"2iQ9O81in8u0"},{"cell_type":"code","execution_count":5,"metadata":{"id":"2ce5c10c","executionInfo":{"status":"ok","timestamp":1650690405644,"user_tz":-270,"elapsed":6,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def initialize_parameters_deep(layer_dims, method='he'):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the dimensions of each layer in our network\n","    method -- string; thte initialization mathod (zeros, random, he)\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","                    bl -- bias vector of shape (layer_dims[l], 1)\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layer_dims) # number of layers in the network\n","\n","    for l in range(1, L):\n","\n","        # zero initialization\n","        if method == 'zeros':\n","          parameters['W' + str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\n","\n","        # random initialization \n","        elif method == 'random':\n","          parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n","        \n","        # he initialization is recommended for relu activation layers\n","        elif method == 'he':\n","          parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2./layer_dims[l-1])\n","\n","        # Initializaing bias to zeros is fine\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","\n","        \n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","        \n","    return parameters"],"id":"2ce5c10c"},{"cell_type":"markdown","source":["###EMA"],"metadata":{"id":"biJb-2jQn_IH"},"id":"biJb-2jQn_IH"},{"cell_type":"code","source":["def initialize_EMA(parameters):\n","  \"\"\"\n","  Initialize EMA (Exponentially moving averages)\n","\n","  Arguments:\n","  parameters -- python dictionary, model weights \n","\n","  Returns:\n","  EMA -- python dictionary containing moving averages\n","  \"\"\"\n","\n","  L = len(parameters) // 2 # number of layers\n","  EMA ={} # \n","  for l in range(L):\n","        EMA['VdW' + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape) # Same shape as parameters W1, ...\n","        EMA['Vdb' + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape) # Same shape as parameters b1, ...\n","\n","  return EMA"],"metadata":{"id":"lLnSmEN6oCdC","executionInfo":{"status":"ok","timestamp":1650690405645,"user_tz":-270,"elapsed":7,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"id":"lLnSmEN6oCdC","execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["###RMS"],"metadata":{"id":"5VKbM2EqoCzN"},"id":"5VKbM2EqoCzN"},{"cell_type":"code","source":["def initialize_RMS(parameters):\n","  \"\"\"\n","  Initialize RMS (Root mean squares)\n","\n","  Arguments:\n","  parameters -- python dictionary, model weights \n","\n","  Returns:\n","  RMS -- python dictionary containing root mean squares\n","  \"\"\"\n","\n","  L = len(parameters) // 2 # number of layers\n","  RMS ={} # \n","  for l in range(L):\n","        RMS['SdW' + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape) # Same shape as parameters W1, ...\n","        RMS['Sdb' + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape) # Same shape as parameters b1, ...\n","\n","  return RMS"],"metadata":{"id":"2OqQZxidoFQL","executionInfo":{"status":"ok","timestamp":1650690405645,"user_tz":-270,"elapsed":6,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"id":"2OqQZxidoFQL","execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9boRgT8AlVld"},"source":["##Forward propagation"],"id":"9boRgT8AlVld"},{"cell_type":"code","execution_count":8,"metadata":{"id":"9658578c","executionInfo":{"status":"ok","timestamp":1650690405645,"user_tz":-270,"elapsed":6,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def linear_forward(A, W, b):\n","    \"\"\"\n","    Implement the linear part of a layer's forward propagation.\n","\n","    Arguments:\n","    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","\n","    Returns:\n","    Z -- the input of the activation function, also called pre-activation parameter \n","    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","    # linear calculation \n","    Z = np.dot(W, A) + b\n","\n","    assert(Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b)\n","    \n","    return Z, cache"],"id":"9658578c"},{"cell_type":"code","execution_count":9,"metadata":{"id":"bc3900cd","executionInfo":{"status":"ok","timestamp":1650690405645,"user_tz":-270,"elapsed":6,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Implement the forward propagation for the LINEAR->ACTIVATION layer\n","\n","    Arguments:\n","    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","\n","    Returns:\n","    A -- the output of the activation function, also called the post-activation value \n","    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n","             stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    if activation == \"sigmoid\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = sigmoid(Z)\n","\n","    \n","    elif activation == \"relu\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","\n","    elif activation == \"softmax\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = softmax(Z)\n","    \n","    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"],"id":"bc3900cd"},{"cell_type":"code","execution_count":10,"metadata":{"id":"81478ca7","executionInfo":{"status":"ok","timestamp":1650690405646,"user_tz":-270,"elapsed":7,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def L_model_forward(X, parameters, keep_prob=1):\n","    \"\"\"\n","    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n","    \n","    Arguments:\n","    X -- data, numpy array of shape (input size, number of examples)\n","    parameters -- output of initialize_parameters_deep()\n","    keep_prob -- dropout probability\n","    \n","    Returns:\n","    AL -- last post-activation value\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n","    \"\"\"\n","\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2 # number of layers in the neural network\n","    masks = {} # To save masks for dropout\n","    \n","    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n","    for l in range(1, L):\n","        A_prev = A \n","        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b'+ str(l)], activation='relu')\n","        A, masks['D' + str(l)] = dropout(A, keep_prob)\n","        caches.append(cache)\n","\n","    \n","    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.    \n","    if parameters['W' + str(L)].shape[0] != 1:\n","      AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation='softmax')\n","\n","    else:\n","      AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation='sigmoid')\n","    caches.append(cache)\n","    \n","    assert(AL.shape == (parameters['W' + str(L)].shape[0],X.shape[1]))\n","            \n","    return AL, caches, masks"],"id":"81478ca7"},{"cell_type":"markdown","metadata":{"id":"1WIRytZvldw1"},"source":["##Cost function"],"id":"1WIRytZvldw1"},{"cell_type":"code","execution_count":11,"metadata":{"id":"bd33eabd","executionInfo":{"status":"ok","timestamp":1650690405646,"user_tz":-270,"elapsed":6,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def compute_cost(AL, Y, parameters=None, lambd=0):\n","    \"\"\"\n","    Implement the cost function defined by equation (7).\n","\n","    Arguments:\n","    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n","    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n","    parameters -- to use in regularization term\n","    lamd -- regularization parameter\n","\n","    Returns:\n","    cost -- cross-entropy cost\n","    \"\"\"\n","    \n","    m = Y.shape[1] # number of examples\n","    L2_regularization_cost = 0\n","    if isinstance(parameters, dict) and lambd:\n","      L = len(parameters) // 2  # Number of layers\n","      weights_square_sum = 0 # Sum of squared weights\n","\n","      # Loop to sum all weights\n","      for l in range(L):\n","        weights_square_sum += np.sum(np.square(parameters['W' + str(l+1)])) \n","\n","      # Regularization term\n","      L2_regularization_cost = (lambd/(2*m)) * weights_square_sum\n","\n","    # Compute loss from aL and y.\n","    if Y.shape[0] != 1:\n","      cross_entropy_cost = -1 / m * np.sum(Y * np.log(AL))\n","    else:\n","      cross_entropy_cost = -1 / m * np.sum(Y * np.log(AL) + (1-Y) * np.log(1-AL))\n","\n","    # Cost \n","    cost = cross_entropy_cost + L2_regularization_cost\n","    cost = np.squeeze(cost) # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n","    assert(cost.shape == ())\n","    \n","    return cost"],"id":"bd33eabd"},{"cell_type":"markdown","metadata":{"id":"bE3Qt6VVlho4"},"source":["##Backward propagation"],"id":"bE3Qt6VVlho4"},{"cell_type":"code","execution_count":12,"metadata":{"id":"6ba01888","executionInfo":{"status":"ok","timestamp":1650690405646,"user_tz":-270,"elapsed":5,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def linear_backward(dZ, cache):\n","    \"\"\"\n","    Implement the linear portion of backward propagation for a single layer (layer l)\n","\n","    Arguments:\n","    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n","    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n","\n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1] # number of examples\n","\n","    dW = 1. / m * np.dot(dZ, A_prev.T) # partial derivative of cost function w.r.t parameter W\n","    db = 1. / m * np.sum(dZ, axis=1, keepdims=True) # partial derivative of cost function w.r.t parameter b\n","    dA_prev = np.dot(W.T, dZ) # partial derivative of cost function w.r.t A (previous layer output)\n","    \n","    assert (dA_prev.shape == A_prev.shape)\n","    assert (dW.shape == W.shape)\n","    assert (db.shape == b.shape)\n","    \n","    return dA_prev, dW, db"],"id":"6ba01888"},{"cell_type":"code","execution_count":13,"metadata":{"id":"937664c3","executionInfo":{"status":"ok","timestamp":1650690405646,"user_tz":-270,"elapsed":5,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def linear_activation_backward(dA, cache, activation, AL=None, Y=None):\n","    \"\"\"\n","    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n","    \n","    Arguments:\n","    dA -- post-activation gradient for current layer l \n","    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","    \n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    linear_cache, activation_cache = cache\n","    \n","    # Derivative w.r.t the activation\n","    if activation == \"relu\":\n","      dZ = relu_backward(dA, activation_cache)\n","      dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","\n","  \n","    elif activation == \"sigmoid\":\n","      dZ = sigmoid_backward(dA, activation_cache)\n","      dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","\n","    elif activation == 'softmax':\n","      dZ = AL - Y\n","      dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","    \n","    return dA_prev, dW, db"],"id":"937664c3"},{"cell_type":"code","execution_count":14,"metadata":{"id":"c2dad4ed","executionInfo":{"status":"ok","timestamp":1650690406003,"user_tz":-270,"elapsed":362,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def L_model_backward(AL, Y, caches, parameters, masks=None, keep_prob=1, lambd=0):\n","    \"\"\"\n","    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n","    \n","    Arguments:\n","    AL -- probability vector, output of the forward propagation (L_model_forward())\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n","                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n","    parameters -- weights to use in regularization term\n","    lambd -- regularization parameter\n","    masks -- python dictionary containing masks used in dropout\n","    keep_prob -- number between 0 and 1 , probability of dropout \n","\n","    Returns:\n","    grads -- A dictionary with the gradients\n","             grads[\"dA\" + str(l)] = ... \n","             grads[\"dW\" + str(l)] = ...\n","             grads[\"db\" + str(l)] = ... \n","    \"\"\"\n","    grads = {}\n","    L = len(caches) # the number of layers\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n","    \n","    # Initializing the backpropagation\n","    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n","    \n","    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n","    current_cache = caches[L-1]\n","    if parameters['W' + str(L)].shape[0] != 1:\n","      grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'softmax', AL=AL, Y=Y)\n","    else:  \n","      grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid')\n","    \n","    # Loop from l=L-2 to l=0\n","    for l in reversed(range(L-1)):\n","        # lth layer: (RELU -> LINEAR) gradients.\n","        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, 'relu')\n","        if masks and (keep_prob != 1) and l > 0:\n","          grads[\"dA\" + str(l)] = (dA_prev_temp * masks['D' + str(l)])/keep_prob\n","        else:\n","           grads[\"dA\" + str(l)] = dA_prev_temp\n","\n","        grads[\"dW\" + str(l + 1)] = dW_temp + (lambd*parameters['W' + str(l+1)]/m)\n","        grads[\"db\" + str(l + 1)] = db_temp\n","\n","    return grads"],"id":"c2dad4ed"},{"cell_type":"markdown","source":["##Learning rate update"],"metadata":{"id":"PaoqAsc94e99"},"id":"PaoqAsc94e99"},{"cell_type":"code","source":["def update_learning_rate(learning_rate0, epoch_num, decay_rate=1):\n","    \"\"\"\n","    Calculates updated the learning rate using exponential weight decay.\n","    \n","    Arguments:\n","    learning_rate0 -- Original learning rate. Scalar\n","    epoch_num -- Epoch number. Integer\n","    decay_rate -- Decay rate. Scalar\n","\n","    Returns:\n","    learning_rate -- Updated learning rate. Scalar \n","    \"\"\"\n","    \n","    learning_rate = learning_rate0 / (1 + decay_rate * epoch_num)\n","    \n","    return learning_rate"],"metadata":{"id":"WhftUttN4ihv","executionInfo":{"status":"ok","timestamp":1650690406004,"user_tz":-270,"elapsed":8,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"id":"WhftUttN4ihv","execution_count":15,"outputs":[]},{"cell_type":"markdown","source":["##Dropout"],"metadata":{"id":"aDoZ1o1KbNKo"},"id":"aDoZ1o1KbNKo"},{"cell_type":"code","source":["def dropout(A, keep_prob):\n","  \"\"\"\n","  Drops out some hidden units to regularize \n","\n","  Arguments:\n","  A -- output of a layer \n","  keep_prob -- probability of dropout\n","\n","  returns:\n","  A_new -- the A matrix with applied dropout\n","  \n","  \"\"\"\n","  D = np.random.randn(*A.shape) # creating a random array \n","  D = (D < keep_prob) # set values to 0 (if larger than keep_prob) and 1 (if smaller than keep_prob)\n","  A_new = np.multiply(A, D) # shut down some values\n","  A_new = A_new / keep_prob # scale the values that haven't been shut down\n","  \n","  return A_new, D"],"metadata":{"id":"BwLs4BKYbTC6","executionInfo":{"status":"ok","timestamp":1650690406004,"user_tz":-270,"elapsed":7,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"id":"BwLs4BKYbTC6","execution_count":16,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WimNpQCdll48"},"source":["## Update prameters"],"id":"WimNpQCdll48"},{"cell_type":"code","execution_count":17,"metadata":{"id":"203a45e8","executionInfo":{"status":"ok","timestamp":1650690406004,"user_tz":-270,"elapsed":6,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def update_parameters(parameters, grads, learning_rate, optimizer='gd', EMA=None, RMS=None, beta1=0.9, beta2=0.999, counter=None):\n","    \"\"\"\n","    Update parameters using optimizer ('defualt: 'gradiend descent')\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    grads -- python dictionary containing your gradients, output of L_model_backward\n","    learning_rate -- small number (alpha)\n","    optimizer -- optimization algorithm ('gd', 'momentum', 'rmsprop', 'adam')\n","    EMA -- python dictionary containing exponentially (weighted) moving averages\n","    RMS -- python dictionary containing root mean squared (weighted) moving averages\n","    beta1 -- parameter for EMA (used in 'momentum' and 'adam')\n","    beta2 -- parameter for RMS (used in 'rmsprop' and 'adam')\n","\n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","                  parameters[\"W\" + str(l)] = ... \n","                  parameters[\"b\" + str(l)] = ...\n","    RMS, EMA -- python dictionary containing moving averages\n","    \"\"\"\n","    \n","    L = len(parameters) // 2 # number of layers in the neural network\n","\n","    # Update rule for each parameter. Use a for loop.\n","\n","    # Gradient descent\n","    if optimizer == 'gd':\n","      for l in range(L):\n","          parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n","          parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n","      return parameters\n","\n","    # Gradient descent with momentum\n","    elif optimizer == 'momentum':\n","      for l in range(L):\n","        # Compute exponentially (weighted) moving averages\n","        EMA['VdW' + str(l+1)] = np.multiply(beta1, EMA['VdW' + str(l+1)]) + np.multiply(1-beta1, grads['dW' + str(l+1)])\n","        EMA['Vdb' + str(l+1)] = np.multiply(beta1, EMA['Vdb' + str(l+1)]) + np.multiply(1-beta1, grads['db' + str(l+1)])\n","        \n","        # Update parameters\n","        parameters[\"W\" + str(l+1)] -= learning_rate * EMA[\"VdW\" + str(l+1)]\n","        parameters[\"b\" + str(l+1)] -= learning_rate * EMA[\"Vdb\" + str(l+1)]\n","      return (parameters, EMA)\n","    \n","    # Root mean squared propagation\n","    elif optimizer == 'rmsprop':\n","      epsilon = 1e-8\n","      for l in range(L):\n","        # Compute root mean squared\n","        RMS['SdW' + str(l+1)] = beta2*RMS['SdW' + str(l+1)] + (1-beta2)*grads['dW' + str(l+1)]**2\n","        RMS['Sdb' + str(l+1)] = beta2*RMS['Sdb' + str(l+1)] + (1-beta2)*grads['db' + str(l+1)]**2\n","\n","        # Update parameters\n","        parameters[\"W\" + str(l+1)] -= (learning_rate * grads['dW' + str(l+1)]) / (np.sqrt(RMS[\"SdW\" + str(l+1)]) + epsilon)\n","        parameters[\"b\" + str(l+1)] -= (learning_rate * grads['db' + str(l+1)]) / (np.sqrt(RMS[\"Sdb\" + str(l+1)]) + epsilon) \n","      return (parameters, RMS)\n","    \n","    # Adam optimization algorithm\n","    elif optimizer == 'adam':\n","      v_corrected = {} # Initializing first moment estimate, python dictionary\n","      s_corrected = {} # Initializing second moment estimate, python dictionary \n","      epsilon = 1e-08\n","      for l in range(L):\n","        # Compute exponentially (weighted) moving averages\n","        EMA['VdW' + str(l+1)] = np.multiply(beta1, EMA['VdW' + str(l+1)]) + np.multiply(1-beta1, grads['dW' + str(l+1)])\n","        EMA['Vdb' + str(l+1)] = np.multiply(beta1, EMA['Vdb' + str(l+1)]) + np.multiply(1-beta1, grads['db' + str(l+1)])\n","\n","        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\"\n","        v_corrected[\"VdW\" + str(l + 1)] = EMA[\"VdW\" + str(l + 1)] / (1 - beta1**counter)\n","        v_corrected[\"Vdb\" + str(l + 1)] = EMA[\"Vdb\" + str(l + 1)] / (1 - beta1**counter)\n","\n","        # Compute root mean squared\n","        RMS['SdW' + str(l+1)] = beta2*RMS['SdW' + str(l+1)] + (1-beta2)*grads['dW' + str(l+1)]**2\n","        RMS['Sdb' + str(l+1)] = beta2*RMS['Sdb' + str(l+1)] + (1-beta2)*grads['db' + str(l+1)]**2\n","\n","        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\"\n","        s_corrected[\"SdW\" + str(l + 1)] = RMS[\"SdW\" + str(l + 1)] / (1 - beta2**counter)\n","        s_corrected[\"Sdb\" + str(l + 1)] = RMS[\"Sdb\" + str(l + 1)] / (1 - beta2**counter)\n","\n","        # Update parameters\n","        parameters[\"W\" + str(l+1)] -= (learning_rate * v_corrected[\"VdW\" + str(l+1)]) / (np.sqrt(s_corrected[\"SdW\" + str(l+1)]) + epsilon)\n","        parameters[\"b\" + str(l+1)] -= (learning_rate * v_corrected[\"Vdb\" + str(l+1)]) / (np.sqrt(s_corrected[\"Sdb\" + str(l+1)]) + epsilon)\n","      return (parameters, EMA, RMS)\n"],"id":"203a45e8"},{"cell_type":"markdown","metadata":{"id":"D7fY754FoAKs"},"source":["##Gradient check"],"id":"D7fY754FoAKs"},{"cell_type":"code","execution_count":18,"metadata":{"id":"XYZHOVyioFbq","executionInfo":{"status":"ok","timestamp":1650690406005,"user_tz":-270,"elapsed":7,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def gradient_check(X, Y, layers_dims, epsilon=1e-7, print_iter=False):\n","  \"\"\"\n","  Checks if the gradient computing (back propagation) is working properly\n","\n","  Arguments:\n","  X --> data (inputs), numpy array of size (number of examples, num_px * num_px * 3)\n","  Y --> Ground truth labels, a numpy array of size (1, number of examples)\n","  layer_dims --> dimension of our network, python list\n","  epsilon --> a small number to use in derivative \n","  print_iter --> print iterated number each print_iter iterations, False to not print, number to print\n","\n","  Returns:\n","  diff --> difference of computed gradients, real number\n","  error --> the difference of diff and epsilon\n","  \"\"\"\n","  parameters = initialize_parameters_deep(layers_dims)\n","\n","  L = len(parameters) // 2 # number pf layers \n","\n","  AL, caches, _ = L_model_forward(X, parameters)\n","  grads =  L_model_backward(AL, Y, caches, parameters)\n","  dtheta = to_vector(grads)\n","\n","  shapes = get_shapes(parameters) # get the shapes\n","  size = 0 # size of column vector\n","  \n","  for shape in shapes:\n","    size += shape[0] * shape[1]\n","\n","  dtheta_approx = np.zeros((size,1)) # initialize vector of zeros\n","  assert(dtheta.shape == dtheta_approx.shape)\n","  \n","  print(size)\n","  for i in range(size):\n","    if print_iter:\n","      if i % print_iter == 0:\n","        print(i)\n","    theta = to_vector(parameters)\n","\n","    # add the epsilon\n","    theta[i] += epsilon\n","\n","    # turn to object\n","    params = from_vector(theta, shapes)\n","\n","    # forward propagation\n","    AL, caches, _ = L_model_forward(X, params)\n","\n","    # compute cost \n","    cost_pos = compute_cost(AL, Y)\n","\n","    # subtract the epsilon\n","    theta[i] -= 2 * epsilon\n","\n","    # turn to object\n","    params = from_vector(theta, shapes)\n","    assert(theta.shape == dtheta_approx.shape)\n","    \n","    # forward propagation\n","    AL, caches, _ = L_model_forward(X, params)\n","\n","    # compute cost \n","    cost_neg = compute_cost(AL, Y)\n","\n","    # assign to dtheta_approx\n","    dtheta_approx[i] = (cost_pos - cost_neg) / (2 * epsilon)\n","\n","  assert(dtheta.shape == dtheta_approx.shape)\n","  # Compute the difference of calculated gradients\n","  diff = np.linalg.norm(dtheta_approx - dtheta) / (np.linalg.norm(dtheta_approx) + np.linalg.norm(dtheta))\n","  error = np.linalg.norm(epsilon - diff) # Error w.r.t epsilon\n","\n","  if diff > 2e-7:\n","    print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(diff) + \"\\033[0m\")\n","  else:\n","    print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(diff) + \"\\033[0m\")\n","  return diff, error"],"id":"XYZHOVyioFbq"},{"cell_type":"code","execution_count":19,"metadata":{"id":"rCpsTUiKvJev","executionInfo":{"status":"ok","timestamp":1650690406005,"user_tz":-270,"elapsed":6,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def get_shapes(parameters):\n","  \"\"\"\n","  Gets the shapes of the paramters \n","\n","  Arguments:\n","  parameters --> an object containing all the parameters W1, b1 , ....\n","\n","  Retruns:\n","  shapes --> python list containing all parameters shapes\n","  \"\"\"\n","  shapes = [] # save shapes for later use\n","  L = len(parameters) // 2 # number of layers \n","\n","  for l in range(L):\n","    shapes.append(parameters['W' + str(l+1)].shape)\n","    shapes.append(parameters['b' + str(l+1)].shape)\n","  \n","  return shapes\n"],"id":"rCpsTUiKvJev"},{"cell_type":"code","execution_count":20,"metadata":{"id":"jGNa07p7jUsX","executionInfo":{"status":"ok","timestamp":1650690406005,"user_tz":-270,"elapsed":6,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def to_vector(parameters):\n","  \"\"\"\n","  Gets all the parameters and turns them into a column vector\n","\n","  Arguments:\n","  parameters --> an object containing all the parameters W1, b1 , ....\n","\n","  Returns:\n","  theta --> the column vector containing parameters \n","  \"\"\"\n","\n","  L = len(parameters) // 2 # number of layers \n","  vectors = []\n","  for l in range(L):\n","    if 'W' + str(l+1) in parameters:\n","      # append parameters (W, b) of layer l to vectors\n","      # array.reshape(-1, 1) reshapes as a column vector\n","      vectors.append(parameters['W' + str(l+1)].reshape(-1,1)) \n","      vectors.append(parameters['b' + str(l+1)].reshape(-1,1))\n","\n","    elif 'dW' + str(l+1) in parameters:\n","      # append grads (dW, db) of layer l to vectors\n","      # array.reshape(-1, 1) reshapes as a column vector\n","      vectors.append(parameters['dW' + str(l+1)].reshape(-1,1))\n","      vectors.append(parameters['db' + str(l+1)].reshape(-1,1))\n","      \n","  theta = np.concatenate(vectors, axis=0) # concatenate vectors\n","\n","  return theta"],"id":"jGNa07p7jUsX"},{"cell_type":"code","execution_count":21,"metadata":{"id":"V348G_Fol6M5","executionInfo":{"status":"ok","timestamp":1650690406006,"user_tz":-270,"elapsed":7,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def from_vector(theta, shapes):\n","  \"\"\"\n","  Turns theta column vector to previous arrays with previous shapes\n","\n","  Arguments:\n","  theta --> column vector containing all parameters , numpy array\n","  shapes --> python list with tupled shapes in each index refering to the parameters \n","\n","  Returns:\n","  parameters --> python object containing all parameters W1, ..... \n","  \"\"\"\n","  shapes = copy.deepcopy(shapes)\n","  parameters = {}\n","  L = len(shapes) // 2 # Number of layers\n","\n","  for l in range(L):\n","    shape = shapes.pop(0) # shape of parameter W of layer l\n","    parameters['W' + str(l+1)] = theta[0:shape[0]*shape[1]].reshape(shape) # reshape as orignial shape\n","    theta = theta[shape[0]*shape[1]:] # eleminate the processed part\n","\n","    shape = shapes.pop(0) # shape of parameter b of layer l\n","    parameters['b' + str(l+1)] = theta[0:shape[0]*shape[1]].reshape(shape) # reshape as orignial shape\n","    theta = theta[shape[0]*shape[1]:] # eleminate the processed part\n","\n","  return parameters\n","  "],"id":"V348G_Fol6M5"},{"cell_type":"markdown","source":["##Mini_batch"],"metadata":{"id":"1wclfWgZq2zW"},"id":"1wclfWgZq2zW"},{"cell_type":"code","source":["def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n","    \"\"\"\n","    Creates a list of random minibatches from (X, Y)\n","    \n","    Arguments:\n","    X -- input data, of shape (input size, number of examples)\n","    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n","    mini_batch_size -- size of the mini-batches, integer\n","    \n","    Returns:\n","    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n","    \"\"\"\n","    \n","    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n","    m = X.shape[1]                  # number of training examples\n","    mini_batches = []\n","    C = Y.shape[0]\n","        \n","    # Step 1: Shuffle (X, Y)\n","    permutation = list(np.random.permutation(m))\n","    shuffled_X = X[:, permutation]\n","    shuffled_Y = Y[:, permutation].reshape((C,m))\n","\n","    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n","    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n","    for k in range(0, num_complete_minibatches):\n","\n","        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n","        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n","\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","    \n","    # Handling the end case (last mini-batch < mini_batch_size)\n","    if m % mini_batch_size != 0:\n","        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n","        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n","\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","    \n","    return mini_batches"],"metadata":{"id":"a7gki3BRq5Km","executionInfo":{"status":"ok","timestamp":1650690406006,"user_tz":-270,"elapsed":6,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"id":"a7gki3BRq5Km","execution_count":22,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"P_yo3RT_mK60"},"source":["##Multi-Layer Perceptron"],"id":"P_yo3RT_mK60"},{"cell_type":"code","execution_count":23,"metadata":{"id":"22f3bb58","executionInfo":{"status":"ok","timestamp":1650690406402,"user_tz":-270,"elapsed":402,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["# GRADED FUNCTION: L_layer_model\n","\n","def L_layer_model(X, Y, layers_dims, optimizer='gd', initialization='he', mini_batch_size=64, learning_rate=0.0075,\n","                  epochs=3000, print_cost=False,keep_prob=1, lambd=0, decay=None, decay_rate=1):\n","    \"\"\"\n","    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n","    \n","    Arguments:\n","    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n","    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n","    optimizer -- optimization algorithm ('gd', 'momentum', 'rmsprop', 'adam')\n","    initialization -- method of initializing parameters ('zero', 'random', 'he')\n","    learning_rate -- learning rate of the gradient descent update rule\n","    num_iterations -- number of iterations of the optimization loop\n","    print_cost -- if True, it prints the cost every 100 steps\n","    decay -- function to update learning rate\n","    decay_rate -- rate of decaying the learning rate , scalar\n","    \n","    Returns:\n","    parameters -- parameters learnt by the model. They can then be used to predict.\n","    \"\"\"\n","    # Alerting the type of optimization based on mini_batch_size\n","    message = 'Batch' if mini_batch_size == X.shape[1] else ('Stochastic' if mini_batch_size == 1 else 'mini-batch')\n","    print(message + ' Gradient Descent')\n","    \n","    np.random.seed(1)\n","    seed = 10 # Seed\n","    minibatches = random_mini_batches(X, Y, mini_batch_size, seed) # mini_batch\n","\n","    t = 0 # Moving Average counter\n","\n","    costs = [] # keep track of cost\n","    learning_rate0 = learning_rate # Saving the initial learning rate\n","    \n","    # Parameters initialization. \n","    parameters = initialize_parameters_deep(layers_dims, method=initialization)\n","    L = len(parameters) // 2\n","\n","    # Check the optimizer and initializing the requirements\n","    # Gradient descent with momentum\n","    if optimizer == 'momentum' or optimizer == 'adam':\n","      EMA = initialize_EMA(parameters) # Exponenetially (weighted) moving averages\n","      \n","    # Root mean square propagation\n","    if optimizer == 'rmsprop' or optimizer == 'adam':\n","      RMS  = initialize_RMS(parameters) # Root mean square\n","      \n","    \n","    # Loop (gradient descent)\n","    for i in range(0, epochs):\n","      seed += 1\n","      if message == 'mini-batch':\n","        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n","\n","      for minibatch in minibatches:\n","        # Select a minibatch\n","        (minibatch_X, minibatch_Y) = minibatch\n","        \n","        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n","        AL, caches, masks = L_model_forward(minibatch_X, parameters, keep_prob=keep_prob)\n","        \n","        # Compute cost.\n","        cost = compute_cost(AL, minibatch_Y, parameters=parameters, lambd=lambd)\n","    \n","        # Backward propagation.\n","        grads = L_model_backward(AL, minibatch_Y, caches, parameters,masks=masks, keep_prob=keep_prob, lambd=lambd)\n","\n","        # Update parameters.\n","        if optimizer == 'gd':\n","          parameters = update_parameters(parameters, grads, learning_rate)\n","        elif optimizer == 'momentum':\n","          parameters, EMA = update_parameters(parameters, grads, learning_rate, optimizer='momentum', EMA=EMA)\n","        elif optimizer == 'rmsprop':\n","          parameters, RMS = update_parameters(parameters, grads, learning_rate, optimizer='rmsprop', RMS=RMS)\n","        elif optimizer == 'adam':\n","          t += 1\n","          parameters, EMA, RMS = update_parameters(parameters, grads, learning_rate, optimizer='adam', RMS=RMS, EMA=EMA, counter=t)\n","      \n","      # Learning rate decay\n","      if decay:\n","        learning_rate = decay(learning_rate0, i, decay_rate)\n","\n","      # Print the cost every 100 training example\n","      if print_cost and i % print_cost == 0:\n","          print (\"Cost after epoch %i: %f\" %(i, cost), end=' ')\n","          costs.append(cost)\n","          if decay:\n","            print(f'|| Learning rate: {learning_rate}')\n","            \n","    # plot the \n","    if print_cost:\n","      plt.plot(np.squeeze(costs))\n","      plt.ylabel('cost')\n","      plt.xlabel('iterations (per tens)')\n","      plt.title(\"Learning rate =\" + str(learning_rate))\n","      plt.show()\n","    \n","    return parameters"],"id":"22f3bb58"},{"cell_type":"markdown","metadata":{"id":"KSOvnnyRlrV9"},"source":["## Predict"],"id":"KSOvnnyRlrV9"},{"cell_type":"code","execution_count":24,"metadata":{"id":"2efc2b53","executionInfo":{"status":"ok","timestamp":1650690406403,"user_tz":-270,"elapsed":4,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["def predict(X, y, parameters):\n","    \"\"\"\n","    This function is used to predict the results of a  L-layer neural network.\n","    \n","    Arguments:\n","    X -- data set of examples you would like to label\n","    parameters -- parameters of the trained model\n","    \n","    Returns:\n","    p -- predictions for the given dataset X\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    n = len(parameters) // 2 # number of layers in the neural network\n","    \n","    # Forward propagation\n","    y_hat, caches, masks = L_model_forward(X, parameters)\n","  \n","    # convert probas to 0/1 predictions\n","    p = np.argmax(y_hat,axis=0).reshape(1,-1)\n","\n","    # inverse the one hot encode\n","    y = np.argmax(y, axis=0).reshape(1,-1)\n","\n","    # count the correctly predicted \n","    true_count = (p == y).sum()\n","    print(f'Correctly predicted: {true_count}')\n","    print(\"Accuracy: \"  + str(true_count / y.shape[1] * 100))\n","        \n","    return p"],"id":"2efc2b53"},{"cell_type":"markdown","source":["##Predict one example"],"metadata":{"id":"lktS-901lmPk"},"id":"lktS-901lmPk"},{"cell_type":"code","source":["def predict_one(parameters, path):\n","  \"\"\"\n","  Predicts one input from a path \n","\n","  Arguments:\n","  x -- \n","  parameters -- learned weights \n","  path -- path to get picture from\n","\n","  Returns:\n","  predicted_image -- predicted label\n","  \"\"\"\n","  num_px = int(np.square(parameters['W1'].shape[1]))\n","  image = np.array(plt.imread(path))\n","  plt.imshow(image)\n","  image = np.array(Image.fromarray(image).resize(size=(num_px, num_px))).reshape((num_px*num_px,1))\n","  image /= 255.\n","  predicted_image = predict(image, [1], parameters)\n","\n","  return predicted_image\n","\n"],"metadata":{"id":"ypD3NunhlxaB","executionInfo":{"status":"ok","timestamp":1650690406403,"user_tz":-270,"elapsed":4,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"id":"ypD3NunhlxaB","execution_count":25,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CjfAJoyilv1N"},"source":["## Load data"],"id":"CjfAJoyilv1N"},{"cell_type":"code","execution_count":26,"metadata":{"id":"07775154","scrolled":true,"executionInfo":{"status":"ok","timestamp":1650690406403,"user_tz":-270,"elapsed":4,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["\n","# Cat non-Cat data set\n","def load_cat_data():\n","  \"\"\"\n","  Loads cat dataset\n","\n","  Arguments:\n","  ----\n","\n","  Returns:\n","  X_train_orig -- matrix of train set features\n","  y_train_orig -- matrix of train set labels\n","  X_test_orig -- matrix of test set features\n","  y_test_orig -- matrix of test set labels\n","  classes -- array of all classes\n","  \"\"\"\n","  from google.colab import drive\n","  drive.mount('/content/drive')\n","\n","  train_dataset = h5py.File('/content/drive/MyDrive/Datasets/cat_noncat/train_catvnoncat.h5', \"r\")\n","  X_train = np.array(train_dataset[\"train_set_x\"][:]) # your train set features\n","  y_train = np.array(train_dataset[\"train_set_y\"][:]) # your train set labels\n","\n","  test_dataset = h5py.File('/content/drive/MyDrive/Datasets/cat_noncat/test_catvnoncat.h5', \"r\")\n","  X_test = np.array(test_dataset[\"test_set_x\"][:]) # your test set features\n","  y_test = np.array(test_dataset[\"test_set_y\"][:]) # your test set labels\n","\n","  classes = np.array(test_dataset[\"list_classes\"][:]) # the list of classes\n","  \n","  y_train = y_train.reshape((1, y_train.shape[0]))\n","  y_test = y_test.reshape((1, y_test.shape[0]))\n","  \n","  return X_train, y_train, X_test, y_test, classes\n","\n","\n","# MNIST (digits) dataset\n","def load_MNIST_data():\n","  \"\"\"\n","  Loads the MNIST dataset (Multi-class)\n","\n","  Arguments:\n","  -------\n","\n","  Returns:\n","  X_train -- matrix of train set features\n","  y_train -- matrix of train set labels\n","  X_test -- matrix of test set features\n","  y_test -- matrix of test set labels\n","  classes -- array of all classes\n","  \"\"\"\n","  df_train = pd.read_csv('sample_data/mnist_train_small.csv', header=None)\n","  df_test = pd.read_csv('sample_data/mnist_test.csv', header=None)\n","\n","  X_train = np.array(df_train.drop([0], axis=1)) # Train set features\n","  X_train = X_train.reshape(X_train.shape[0], 28,28)\n","  y_train = np.array(df_train[0]).reshape(1,-1) # Train set labels\n","  X_test = np.array(df_test.drop([0], axis=1)) # Test set features\n","  X_test = X_test.reshape(X_test.shape[0], 28,28)\n","  y_test = np.array(df_test[0]).reshape(1,-1) # Test set labels\n","\n","  classes = np.unique(y_test) # All the classes\n","\n","  return X_train, y_train, X_test, y_test, classes\n","\n"],"id":"07775154"},{"cell_type":"markdown","source":["##Save and Load model"],"metadata":{"id":"ooMjM8bscSwQ"},"id":"ooMjM8bscSwQ"},{"cell_type":"code","source":["def save_model(parameters, path):\n","  \"\"\"\n","  pickes the model paramters for later use \n","\n","\n","  Arguments:\n","  paramters -- model weights, python dictionary\n","  path -- path to save model in (picke file), string\n","\n","  Returns:\n","  None\n","  \"\"\"\n","  with open(path, 'wb') as f:\n","    pickle.dump(parameters, f, protocol=pickle.HIGHEST_PROTOCOL)\n","  "],"metadata":{"id":"zVSd7bufcVt_","executionInfo":{"status":"ok","timestamp":1650690406404,"user_tz":-270,"elapsed":4,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"id":"zVSd7bufcVt_","execution_count":27,"outputs":[]},{"cell_type":"code","source":["def load_model(path):\n","  \"\"\"\n","  Loads model from given path \n","\n","  Arguments:\n","  path -- string, path to get model \n","\n","  Returns:\n","  parameters -- model parameters \n","  \"\"\"\n","\n","  with open(path, 'rb') as f:\n","    parameters = pickle.load(f)\n","\n","  return parameters"],"metadata":{"id":"eRKT_xFseCrw","executionInfo":{"status":"ok","timestamp":1650690406404,"user_tz":-270,"elapsed":4,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"id":"eRKT_xFseCrw","execution_count":28,"outputs":[]},{"cell_type":"markdown","source":["#Data analysis"],"metadata":{"id":"GsYkIl0v19qh"},"id":"GsYkIl0v19qh"},{"cell_type":"code","execution_count":29,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":282},"executionInfo":{"elapsed":3107,"status":"ok","timestamp":1650690409507,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"},"user_tz":-270},"id":"3adc21f4","outputId":"985cc5f3-6ed5-40f1-d38e-ae6816692adf"},"outputs":[{"output_type":"stream","name":"stdout","text":["It's the digit 2 \n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["X_train, y_train, X_test, y_test, classes = load_MNIST_data()\n","# Example of a picture\n","index = 10\n","if len(classes) == 2:\n","  plt.imshow(X_train[index])\n","  print (\"y = \" + str(y_train[0,index]) + \". It's a \" + classes[y_train[0,index]].decode(\"utf-8\") +  \" picture.\")\n","else:\n","  plt.imshow(X_train[index], cmap='gray')\n","  print(f'It\\'s the digit {y_train[0,index]} ')"],"id":"3adc21f4"},{"cell_type":"markdown","metadata":{"id":"KgixX6xvl51N"},"source":["###Exploring dataset"],"id":"KgixX6xvl51N"},{"cell_type":"code","execution_count":30,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1650690409507,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"},"user_tz":-270},"id":"18284bb6","outputId":"fe4c2639-b096-4d5b-bd07-32a7fea544ae"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of training examples: 20000\n","Number of testing examples: 10000\n","Each image is of size: (28, 28, 3)\n","X_train shape: (20000, 28, 28)\n","y_train shape: (1, 20000)\n","X_test shape: (10000, 28, 28)\n","y_test shape: (1, 10000)\n"]}],"source":["# Explore your dataset \n","m_train = X_train.shape[0]\n","num_px = X_train.shape[1]\n","m_test = X_test.shape[0]\n","\n","print (\"Number of training examples: \" + str(m_train))\n","print (\"Number of testing examples: \" + str(m_test))\n","print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n","print (\"X_train shape: \" + str(X_train.shape))\n","print (\"y_train shape: \" + str(y_train.shape))\n","print (\"X_test shape: \" + str(X_test.shape))\n","print (\"y_test shape: \" + str(y_test.shape))\n"],"id":"18284bb6"},{"cell_type":"markdown","metadata":{"id":"X2kz6FcgmAc5"},"source":["###Reshaping and normalizing"],"id":"X2kz6FcgmAc5"},{"cell_type":"code","execution_count":31,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1650690409508,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"},"user_tz":-270},"id":"6bf2e100","outputId":"2f3a155f-a567-4a86-d158-39846e055226"},"outputs":[{"output_type":"stream","name":"stdout","text":["X_train's shape: (784, 20000)\n","X_test's shape: (784, 10000)\n"]}],"source":["# Reshape the training and test examples \n","X_train_flatten = X_train.reshape(X_train.shape[0], -1).T   # The \"-1\" makes reshape flatten the remaining dimensions\n","X_test_flatten = X_test.reshape(X_test.shape[0], -1).T\n","\n","# Standardize data to have feature values between 0 and 1.\n","X_train = X_train_flatten/255.\n","X_test = X_test_flatten/255.\n","\n","print (\"X_train's shape: \" + str(X_train.shape))\n","print (\"X_test's shape: \" + str(X_test.shape))"],"id":"6bf2e100"},{"cell_type":"markdown","source":["###Label Encoding"],"metadata":{"id":"-Zvgh2qP6Vnl"},"id":"-Zvgh2qP6Vnl"},{"cell_type":"code","source":["if len(classes) > 2:\n","  # Encoding labels\n","  lb = LabelBinarizer()\n","  y_train = lb.fit_transform(y_train.T).T\n","  y_test = lb.fit_transform(y_test.T).T\n","  print(\"New shapes:\")\n","  print('y_train dimension: ' + str(y_train.shape))\n","  print('y_test dimension: ' + str(y_test.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"buKT5E-l6Zgx","executionInfo":{"status":"ok","timestamp":1650690409508,"user_tz":-270,"elapsed":8,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}},"outputId":"46b31071-fdec-4c27-bbf7-fcdeef47c176"},"id":"buKT5E-l6Zgx","execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["New shapes:\n","y_train dimension: (10, 20000)\n","y_test dimension: (10, 10000)\n"]}]},{"cell_type":"markdown","source":["#Applying Gradient check"],"metadata":{"id":"9Itn7xQSlKqo"},"id":"9Itn7xQSlKqo"},{"cell_type":"code","source":["# shrink the data to use in gradient check\n","x_grad_check = X_train[:700,:200] \n","y_grad_check = y_train[:,:200]\n","print('x shape:' + str(x_grad_check.shape))\n","print('y shape:' + str(y_grad_check.shape))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M-a2eTZ83p1M","executionInfo":{"status":"ok","timestamp":1650690409509,"user_tz":-270,"elapsed":8,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}},"outputId":"7507098b-bbe3-4147-e16f-d8533e44ac69"},"id":"M-a2eTZ83p1M","execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["x shape:(700, 200)\n","y shape:(10, 200)\n"]}]},{"cell_type":"code","execution_count":34,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1650690409509,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"},"user_tz":-270},"id":"yACNxLrjcNf4"},"outputs":[],"source":["dims = [700, 20, 10]\n","# prob, diff = gradient_check(x_grad_check, y_grad_check, layers_dims=dims, print_iter=10000) "],"id":"yACNxLrjcNf4"},{"cell_type":"markdown","metadata":{"id":"iPUgK7sWm3yD"},"source":["#ِDefining model "],"id":"iPUgK7sWm3yD"},{"cell_type":"code","execution_count":35,"metadata":{"id":"da029b7e","executionInfo":{"status":"ok","timestamp":1650690409509,"user_tz":-270,"elapsed":8,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["layers_dims = [784, 128, 10] #  3-layer model\n","\n"],"id":"da029b7e"},{"cell_type":"markdown","metadata":{"id":"QaKxTOiVnFCc"},"source":["#Train"],"id":"QaKxTOiVnFCc"},{"cell_type":"code","execution_count":36,"metadata":{"id":"z4H8box4nECp","executionInfo":{"status":"ok","timestamp":1650690409510,"user_tz":-270,"elapsed":9,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["# for solver in ['gd', 'momentum', 'rmsprop', 'adam']:\n","#   s = time.time()\n","#   parameters = L_layer_model(train_x, train_y, layers_dims, epochs=2500,learning_rate=0.001, print_cost = False , optimizer=solver)\n","#   f = time.time()\n","#   print(f'{solver} in {f-s} seconds')\n"],"id":"z4H8box4nECp"},{"cell_type":"code","source":["# parameters = L_layer_model(X_train, y_train, layers_dims, initialization='he', epochs=300, mini_batch_size=1024, learning_rate=0.00075,\n","                          #  decay=update_learning_rate, lambd=1,keep_prob=0.67, decay_rate=0.01, print_cost=100 , optimizer='adam')"],"metadata":{"id":"FLnY6DgrzR2a","executionInfo":{"status":"ok","timestamp":1650690409510,"user_tz":-270,"elapsed":8,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"id":"FLnY6DgrzR2a","execution_count":37,"outputs":[]},{"cell_type":"code","source":["# save_model(parameters, 'MNIST.pickle')"],"metadata":{"id":"96IKW_tdnHJj","executionInfo":{"status":"ok","timestamp":1650690409510,"user_tz":-270,"elapsed":8,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"id":"96IKW_tdnHJj","execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gv9-VpO5nPel"},"source":["#Predicting"],"id":"gv9-VpO5nPel"},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ue9NOdlVYcf3","executionInfo":{"status":"ok","timestamp":1650690428538,"user_tz":-270,"elapsed":19036,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}},"outputId":"89f355ff-c23a-4fb0-fdb8-988d27aa8220"},"id":"Ue9NOdlVYcf3","execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["parameters = load_model(\"/content/drive/MyDrive/Datasets/MNIST/MNIST.pickle\")"],"metadata":{"id":"DrlyBRsC0PoB","executionInfo":{"status":"ok","timestamp":1650690458187,"user_tz":-270,"elapsed":786,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"id":"DrlyBRsC0PoB","execution_count":41,"outputs":[]},{"cell_type":"code","execution_count":42,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"89ecb1ed","executionInfo":{"status":"ok","timestamp":1650690462693,"user_tz":-270,"elapsed":781,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}},"outputId":"3508dadc-b6ed-43dc-affb-cad1527f23a7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Correctly predicted: 19510\n","Accuracy: 97.55\n"]}],"source":["pred_train = predict(X_train, y_train, parameters)\n","# print(f'Accuracy sklearn: {accuracy_score(y_train, pred_train)}')\n"],"id":"89ecb1ed"},{"cell_type":"code","execution_count":43,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4a4cfb8a","executionInfo":{"status":"ok","timestamp":1650690653211,"user_tz":-270,"elapsed":328,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}},"outputId":"7f258db2-11a0-421e-e4ae-89f5f2ca9ebf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Correctly predicted: 9574\n","Accuracy: 95.74000000000001\n"]}],"source":["pred_test = predict(X_test, y_test, parameters)\n","# print(f'Accuracy sklearn: {accuracy_score(y_test, pred_test)}')\n"],"id":"4a4cfb8a"},{"cell_type":"code","execution_count":44,"metadata":{"id":"72232265","executionInfo":{"status":"ok","timestamp":1650690656086,"user_tz":-270,"elapsed":987,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}},"colab":{"base_uri":"https://localhost:8080/","height":317},"outputId":"3c5cc442-7074-40e8-ebbb-486e517df973"},"outputs":[{"output_type":"stream","name":"stdout","text":["Correctly predicted: 1\n","Accuracy: 100.0\n","The model predicted as digit 3\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["my_image = \"digit\" # change this to the name of your image file \n","my_label_y = np.zeros((10,1)) # the true class of your image (1 -> cat, 0 -> non-cat)\n","gray_scale = [0.299, 0.587, 0.114]\n","\n","my_label_y[3,0] = 1\n","fname = \"/content/drive/MyDrive/Datasets/MNIST/\" + my_image + str(2) + '.jpg'\n","image = np.array(plt.imread(fname))\n","image = np.dot(image[...,:3],gray_scale)\n","my_image = np.array(Image.fromarray(image).resize(size=(num_px, num_px))).reshape((num_px*num_px,1))\n","my_image = my_image/255.\n","my_predicted_image = predict(my_image, my_label_y, parameters)\n","\n","plt.imshow(my_image.reshape(num_px,num_px), cmap='gray')\n","# print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n","print(f'The model predicted as digit {np.squeeze(my_predicted_image)}')"],"id":"72232265"},{"cell_type":"code","execution_count":null,"metadata":{"id":"We8Ue44MZsQK","executionInfo":{"status":"aborted","timestamp":1650690428920,"user_tz":-270,"elapsed":387,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"outputs":[],"source":["my_image = np.array(Image.fromarray(image).resize(size=(num_px, num_px)))\n","plt.imshow(my_image)"],"id":"We8Ue44MZsQK"},{"cell_type":"code","source":["my_image = \"cat_noncat\" # change this to the name of your image file \n","my_label_y = [1] # the true class of your image (1 -> cat, 0 -> non-cat)\n","\n","num_cat_px = 64\n","\n","fname = \"/content/drive/MyDrive/Datasets/cat_noncat/\" + my_image + str(1) + '.jpg'\n","image = np.array(plt.imread(fname))\n","my_image = np.array(Image.fromarray(image).resize(size=(num_cat_px, num_cat_px))).reshape((num_cat_px*num_cat_px*3,1))\n","print(my_image.shape)\n","my_image = my_image/255.\n","my_predicted_image = predict(my_image, my_label_y, parameters)\n","\n","plt.imshow(image)\n","# print (\"y = \" + str(np.squeeze(my_predicted_image)) + \", your L-layer model predicts a \\\"\" + classes[int(np.squeeze(my_predicted_image)),].decode(\"utf-8\") +  \"\\\" picture.\")\n","print(my_predicted_image)"],"metadata":{"id":"takgaQKSLQDK","executionInfo":{"status":"aborted","timestamp":1650690428920,"user_tz":-270,"elapsed":386,"user":{"displayName":"Sadra Sadeghian","userId":"09493653721184832024"}}},"id":"takgaQKSLQDK","execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["uUPnQGX0lDr1","Kohh8g5XlN5W","9boRgT8AlVld","WimNpQCdll48","D7fY754FoAKs","1wclfWgZq2zW","CjfAJoyilv1N"],"name":"MLP.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}