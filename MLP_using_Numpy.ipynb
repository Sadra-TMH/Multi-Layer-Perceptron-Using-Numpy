{"cells":[{"cell_type":"markdown","id":"39aa86fa","metadata":{"id":"39aa86fa"},"source":["<a name='Sections'></a>\n","#**Sections**\n","* [Imports](#Imports)\n","* [Implementation](#Implementation)\n","  1. [Activation Functions](#Activation-functions)\n","    * [Forward Pass](#Forward-pass)\n","    * [Backward Pass](#Backward-pass)\n","  2. [Initialization](#Initialization)\n","    * [Weights](#Weights)\n","    * [Exponentially Moving Average](#EMA)\n","    * [Root Mean Squared](#RMS)\n","  3. [Forward Propagation](#Forward-propagation)\n","    * [Linear Forward](#Linear-forward)\n","    * [Linear Activation Forward](#Linear-activation-forward)\n","    * [L Layer Forward](#L-layer-forward)\n","  4. [Cost Function](#Cost-function)\n","  5. [Backward Propagation](#Backward-propagation)\n","    * [Linear Backward](#Linear-backward)\n","    * [Linear Activation backward](#Linear-activation-backward)\n","    * [L Layer Backward](#L-layer-backward)\n","  6. [Learning Rate Update](#Learning-rate-update)\n","  7. [Dropout](#Dropout)\n","  8. [Update parameters](#Update-parameters)\n","  9. [Gradient Check](#Gradient-check)\n","    * [Grad Check](#Grad-check)\n","    * [Get Shapes](#Get-shapes)\n","    * [To Vector](#To-vector)\n","    * [From Vector](#From-vector)\n","  10. [Mini-Batch](#Mini_batch)\n","  11. [Multi-Layer Perceptron](#Multi-Layer-Perceptron)\n","  12. [Predict](#Predict)\n","  13. [Save and Load Model](#Save-and-Load-model)\n","    * [Save Model](#Save-model)\n","    * [Load Model](#Load-model)\n","\n"]},{"cell_type":"markdown","id":"0tR7yrpGkm2N","metadata":{"id":"0tR7yrpGkm2N"},"source":["<a name='Imports'></a>\n","#Imports \n","Importing needed libraries:<br>\n","\n","[**Numpy**](https://numpy.org/) --> For mathematical computations.<br>\n","[**Matplotlib**](https://matplotlib.org/) --> For visualizations.<br>\n","[**pickle**](https://docs.python.org/3/library/pickle.html#module-pickle) --> For saving trained model parameters."]},{"cell_type":"code","execution_count":null,"id":"50870812","metadata":{"id":"50870812"},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import copy\n","import math\n","import pickle\n","%matplotlib inline\n"]},{"cell_type":"markdown","id":"tnDcg4E71hRi","metadata":{"id":"tnDcg4E71hRi"},"source":["<a name='Implementation'></a>\n","#Implementaions\n","There are a total of 13 parts that each cover one element of MLP([See sections](#Sections)).<br>\n","There are also sub-elements for some of the elements such as:<br> ***Initialization-->[Weights, EMA, RMS]***.\n","\n"]},{"cell_type":"markdown","id":"uUPnQGX0lDr1","metadata":{"id":"uUPnQGX0lDr1"},"source":["<a name='Activation-functions'></a>\n","##Activation functions\n","Forward and Backward pass of activation functions:<br>\n","* Sigmoid\n","* ReLU(Rectified Linear Unit)\n","* Softmax"]},{"cell_type":"markdown","id":"ayvAYp8YSe5g","metadata":{"id":"ayvAYp8YSe5g"},"source":["<a name='Forward-pass'></a>\n","###Forward pass"]},{"cell_type":"code","execution_count":null,"id":"b2f02e44","metadata":{"id":"b2f02e44"},"outputs":[],"source":["# Needed Functions \n","\n","def sigmoid(Z):\n","    \"\"\"\n","    Implements the sigmoid activation in numpy\n","    \n","    Arguments:\n","    Z -- numpy array of any shape\n","    \n","    Returns:\n","    A -- output of sigmoid(z), same shape as Z\n","    cache -- returns Z as well, useful during backpropagation\n","    \"\"\"\n","    \n","    A = 1/(1+np.exp(-Z))\n","    cache = Z\n","    \n","    return A, cache\n","\n","def relu(Z):\n","    \"\"\"\n","    Implement the RELU function.\n","\n","    Arguments:\n","    Z -- Output of the linear layer, of any shape\n","\n","    Returns:\n","    A -- Post-activation parameter, of the same shape as Z\n","    cache -- a python dictionary containing \"A\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    A = np.maximum(0,Z)\n","    \n","    assert(A.shape == Z.shape)\n","    \n","    cache = Z \n","    return A, cache\n","\n","\n","def softmax(Z):\n","    \"\"\"\n","    Implement Softmax activation (Multi-class classification)\n","\n","    Arguments:\n","    Z -- output of the linear layer\n","\n","    Returns:\n","    A -- Post_activation parameter, the same shape as Z\n","    cache -- Z to use in backprop\n","    \"\"\"\n","\n","    t = np.exp(Z)\n","    A = t / np.sum(t, axis=0)\n","\n","    assert(A.shape == Z.shape)\n","\n","    cache = Z\n","    return A, cache"]},{"cell_type":"markdown","id":"vtiQfFk5Sm-p","metadata":{"id":"vtiQfFk5Sm-p"},"source":["<a name='Backward-pass'></a>\n","###Backward pass\n","Backward pass, is the Gradient of the function.<br>\n","This is to be used in **Back Propagation**."]},{"cell_type":"code","execution_count":null,"id":"T8KKXYw8SpE2","metadata":{"id":"T8KKXYw8SpE2"},"outputs":[],"source":["\n","def relu_backward(dA, cache):\n","    \"\"\"\n","    Implement the backward propagation for a single RELU unit.\n","\n","    Arguments:\n","    dA -- post-activation gradient, of any shape\n","    cache -- 'Z' where we store for computing backward propagation efficiently\n","\n","    Returns:\n","    dZ -- Gradient of the cost with respect to Z\n","    \"\"\"\n","    \n","    Z = cache\n","    dZ = np.array(dA, copy=True) # just converting dz to a correct object.\n","    \n","    # When z <= 0, you should set dz to 0 as well. \n","    dZ[Z <= 0] = 0\n","    \n","    assert (dZ.shape == Z.shape)\n","    \n","    return dZ\n","\n","def sigmoid_backward(dA, cache):\n","    \"\"\"\n","    Implement the backward propagation for a single SIGMOID unit.\n","\n","    Arguments:\n","    dA -- post-activation gradient, of any shape\n","    cache -- 'Z' where we store for computing backward propagation efficiently\n","\n","    Returns:\n","    dZ -- Gradient of the cost with respect to Z\n","    \"\"\"\n","    \n","    Z = cache\n","    \n","    s = 1/(1+np.exp(-Z))\n","    dZ = dA * s * (1-s)\n","    \n","    assert (dZ.shape == Z.shape)\n","    \n","    return dZ\n"]},{"cell_type":"markdown","id":"Kohh8g5XlN5W","metadata":{"id":"Kohh8g5XlN5W"},"source":["<a name='Initialization'></a>\n","##Initialization\n","Initializing to zeros or random values.\n","* Weights (parameters)\n","* EMA (Exponentially Moving Average)\n","* RMS (Root Mean Squared)"]},{"cell_type":"markdown","id":"2iQ9O81in8u0","metadata":{"id":"2iQ9O81in8u0"},"source":["<a name='Weights'></a>\n","###Weights\n","Weights can be initialized in three ways:\n","* He (Recommended)\n","* Zeros (This method will cause symmetry thus nothing will be learned)\n","* Random"]},{"cell_type":"code","execution_count":null,"id":"2ce5c10c","metadata":{"id":"2ce5c10c"},"outputs":[],"source":["def initialize_parameters_deep(layer_dims, method='he'):\n","    \"\"\"\n","    Arguments:\n","    layer_dims -- python array (list) containing the dimensions of each layer in our network\n","    method -- string; thte initialization mathod (zeros, random, he)\n","    \n","    Returns:\n","    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n","                    Wl -- weight matrix of shape (layer_dims[l], layer_dims[l-1])\n","                    bl -- bias vector of shape (layer_dims[l], 1)\n","    \"\"\"\n","    \n","    np.random.seed(3)\n","    parameters = {}\n","    L = len(layer_dims) # number of layers in the network\n","\n","    for l in range(1, L):\n","\n","        # zero initialization\n","        if method == 'zeros':\n","            parameters['W' + str(l)] = np.zeros((layer_dims[l], layer_dims[l-1]))\n","\n","        # random initialization \n","        elif method == 'random':\n","            parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * 0.01\n","        \n","        # he initialization is recommended for relu activation layers\n","        elif method == 'he':\n","            parameters['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1]) * np.sqrt(2./layer_dims[l-1])\n","\n","        # Initializaing bias to zeros is fine\n","        parameters['b' + str(l)] = np.zeros((layer_dims[l], 1))\n","\n","        \n","        assert(parameters['W' + str(l)].shape == (layer_dims[l], layer_dims[l-1]))\n","        assert(parameters['b' + str(l)].shape == (layer_dims[l], 1))\n","\n","        \n","    return parameters"]},{"cell_type":"markdown","id":"biJb-2jQn_IH","metadata":{"id":"biJb-2jQn_IH"},"source":["<a name='EMA'></a>\n","###EMA\n","Exponentially Moving Average.<br>\n","This is to be used in 'Adam' optimizations method."]},{"cell_type":"code","execution_count":null,"id":"lLnSmEN6oCdC","metadata":{"id":"lLnSmEN6oCdC"},"outputs":[],"source":["def initialize_EMA(parameters):\n","    \"\"\"\n","    Initialize EMA (Exponentially moving averages)\n","\n","    Arguments:\n","    parameters -- python dictionary, model weights \n","\n","    Returns:\n","    EMA -- python dictionary containing moving averages\n","    \"\"\"\n","\n","    L = len(parameters) // 2 # number of layers\n","    EMA ={} # \n","    for l in range(L):\n","        EMA['VdW' + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape) # Same shape as parameters W1, ...\n","        EMA['Vdb' + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape) # Same shape as parameters b1, ...\n","\n","    return EMA"]},{"cell_type":"markdown","id":"5VKbM2EqoCzN","metadata":{"id":"5VKbM2EqoCzN"},"source":["<a name='RMS'></a>\n","###RMS\n","Root Mean Squared.<br>\n","This is to be used in 'Adam' and 'RMSProp' optimizations method."]},{"cell_type":"code","execution_count":null,"id":"2OqQZxidoFQL","metadata":{"id":"2OqQZxidoFQL"},"outputs":[],"source":["def initialize_RMS(parameters):\n","    \"\"\"\n","    Initialize RMS (Root mean squares)\n","\n","    Arguments:\n","    parameters -- python dictionary, model weights \n","\n","    Returns:\n","    RMS -- python dictionary containing root mean squares\n","    \"\"\"\n","\n","    L = len(parameters) // 2 # number of layers\n","    RMS ={} # \n","    for l in range(L):\n","        RMS['SdW' + str(l+1)] = np.zeros(parameters['W' + str(l+1)].shape) # Same shape as parameters W1, ...\n","        RMS['Sdb' + str(l+1)] = np.zeros(parameters['b' + str(l+1)].shape) # Same shape as parameters b1, ...\n","\n","    return RMS"]},{"cell_type":"markdown","id":"9boRgT8AlVld","metadata":{"id":"9boRgT8AlVld"},"source":["<a name='Forward-propagation'></a>\n","##Forward propagation\n","Forward Propagation is made of 3 sub-elements:\n","* linear_forward: Computes the linear part --> Z = W.X + b\n","* linear_activation_forward: Feeds the linear_forward output to activation function --> g(Z)\n","* L_model_forward: Propagates through all the layers using linear_forward and linear_activation_forward"]},{"cell_type":"markdown","source":["<a name='Linear-forward'></a>\n","###Linear forward"],"metadata":{"id":"Mu1KVVjJfOTf"},"id":"Mu1KVVjJfOTf"},{"cell_type":"code","execution_count":null,"id":"9658578c","metadata":{"id":"9658578c"},"outputs":[],"source":["def linear_forward(A, W, b):\n","    \"\"\"\n","    Implement the linear part of a layer's forward propagation.\n","\n","    Arguments:\n","    A -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","\n","    Returns:\n","    Z -- the input of the activation function, also called pre-activation parameter \n","    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\n","    \"\"\"\n","    # linear calculation \n","    Z = np.dot(W, A) + b\n","\n","    assert(Z.shape == (W.shape[0], A.shape[1]))\n","    cache = (A, W, b)\n","    \n","    return Z, cache"]},{"cell_type":"markdown","source":["<a name='Linear-activation-forward'></a>\n","###Linear activation forward\n","Activations functions are ([see activation functions](#Forward-pass)):\n","* ReLU: Used mainly in hidden layers\n","* Sigmoid: Used for Binary classification.\n","* Softmax: Used for Multi-class classification"],"metadata":{"id":"PeIuQWOpfVPy"},"id":"PeIuQWOpfVPy"},{"cell_type":"code","execution_count":null,"id":"bc3900cd","metadata":{"id":"bc3900cd"},"outputs":[],"source":["def linear_activation_forward(A_prev, W, b, activation):\n","    \"\"\"\n","    Implement the forward propagation for the LINEAR->ACTIVATION layer\n","\n","    Arguments:\n","    A_prev -- activations from previous layer (or input data): (size of previous layer, number of examples)\n","    W -- weights matrix: numpy array of shape (size of current layer, size of previous layer)\n","    b -- bias vector, numpy array of shape (size of the current layer, 1)\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","\n","    Returns:\n","    A -- the output of the activation function, also called the post-activation value \n","    cache -- a python dictionary containing \"linear_cache\" and \"activation_cache\";\n","             stored for computing the backward pass efficiently\n","    \"\"\"\n","    \n","    if activation == \"sigmoid\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = sigmoid(Z)\n","\n","    \n","    elif activation == \"relu\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = relu(Z)\n","\n","    elif activation == \"softmax\":\n","        # Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\n","        Z, linear_cache = linear_forward(A_prev, W, b)\n","        A, activation_cache = softmax(Z)\n","    \n","    assert (A.shape == (W.shape[0], A_prev.shape[1]))\n","    cache = (linear_cache, activation_cache)\n","\n","    return A, cache"]},{"cell_type":"markdown","source":["<a name='L-layer-forward'></a>\n","###L layer forward\n","According to the half of number of parameters that indicates number of layers,<br>linear_activation_forward is used to compute A for each layer."],"metadata":{"id":"LM8Ge69CfYvO"},"id":"LM8Ge69CfYvO"},{"cell_type":"code","execution_count":null,"id":"81478ca7","metadata":{"id":"81478ca7"},"outputs":[],"source":["def L_model_forward(X, parameters, keep_prob=1):\n","    \"\"\"\n","    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n","\n","    Arguments:\n","    X -- data, numpy array of shape (input size, number of examples)\n","    parameters -- output of initialize_parameters_deep()\n","    keep_prob -- dropout probability\n","\n","    Returns:\n","    AL -- last post-activation value\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() (there are L-1 of them, indexed from 0 to L-1)\n","    \"\"\"\n","\n","    caches = []\n","    A = X\n","    L = len(parameters) // 2 # number of layers in the neural network\n","    masks = {} # To save masks for dropout\n","\n","    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\" list.\n","    for l in range(1, L):\n","        A_prev = A \n","        A, cache = linear_activation_forward(A_prev, parameters['W' + str(l)], parameters['b'+ str(l)], activation='relu')\n","        A, masks['D' + str(l)] = dropout(A, keep_prob)\n","        caches.append(cache)\n","\n","\n","    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.    \n","    if parameters['W' + str(L)].shape[0] != 1:\n","        AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation='softmax')\n","\n","    else:\n","        AL, cache = linear_activation_forward(A, parameters['W' + str(L)], parameters['b' + str(L)], activation='sigmoid')\n","    caches.append(cache)\n","\n","    assert(AL.shape == (parameters['W' + str(L)].shape[0],X.shape[1]))\n","\n","    return AL, caches, masks"]},{"cell_type":"markdown","id":"1WIRytZvldw1","metadata":{"id":"1WIRytZvldw1"},"source":["<a name='Cost-function'></a>\n","##Cost function\n","Computes the Cross-Entropy loss function for all examples.<br>\n","Sum of all losses is the Cost function values."]},{"cell_type":"code","execution_count":null,"id":"bd33eabd","metadata":{"id":"bd33eabd"},"outputs":[],"source":["def compute_cost(AL, Y, parameters=None, lambd=0):\n","    \"\"\"\n","    Implement the cost function defined by equation (7).\n","\n","    Arguments:\n","    AL -- probability vector corresponding to your label predictions, shape (1, number of examples)\n","    Y -- true \"label\" vector (for example: containing 0 if non-cat, 1 if cat), shape (1, number of examples)\n","    parameters -- to use in regularization term\n","    lamd -- regularization parameter\n","\n","    Returns:\n","    cost -- cross-entropy cost\n","    \"\"\"\n","    \n","    m = Y.shape[1] # number of examples\n","    L2_regularization_cost = 0\n","    if isinstance(parameters, dict) and lambd:\n","      L = len(parameters) // 2  # Number of layers\n","      weights_square_sum = 0 # Sum of squared weights\n","\n","      # Loop to sum all weights\n","      for l in range(L):\n","        weights_square_sum += np.sum(np.square(parameters['W' + str(l+1)])) \n","\n","      # Regularization term\n","      L2_regularization_cost = (lambd/(2*m)) * weights_square_sum\n","\n","    # Compute loss from aL and y.\n","    if Y.shape[0] != 1:\n","        cross_entropy_cost = -1 / m * np.sum(Y * np.log(AL))\n","    else:\n","        cross_entropy_cost = -1 / m * np.sum(Y * np.log(AL) + (1-Y) * np.log(1-AL))\n","\n","    # Cost \n","    cost = cross_entropy_cost + L2_regularization_cost\n","    cost = np.squeeze(cost) # To make sure your cost's shape is what we expect (e.g. this turns [[17]] into 17).\n","    assert(cost.shape == ())\n","    \n","    return cost"]},{"cell_type":"markdown","id":"bE3Qt6VVlho4","metadata":{"id":"bE3Qt6VVlho4"},"source":["<a name='Backward-propagation'></a>\n","##Backward propagation\n","Backward Propagation is made of 3 sub-elements:\n","* linear_backward: Computes the derivative of linea part.\n","* linear_activation_backward: Feeds the linear_backward output to the gradient of activation function.\n","* L_model_backward: Propagates backward through all the layers using linear_backward and linear_activation_backward."]},{"cell_type":"markdown","source":["<a name='Linear-backward'></a>\n","###Linear backward\n"],"metadata":{"id":"js_YkpOVfkle"},"id":"js_YkpOVfkle"},{"cell_type":"code","execution_count":null,"id":"6ba01888","metadata":{"id":"6ba01888"},"outputs":[],"source":["def linear_backward(dZ, cache):\n","    \"\"\"\n","    Implement the linear portion of backward propagation for a single layer (layer l)\n","\n","    Arguments:\n","    dZ -- Gradient of the cost with respect to the linear output (of current layer l)\n","    cache -- tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n","\n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    A_prev, W, b = cache\n","    m = A_prev.shape[1] # number of examples\n","\n","    dW = 1. / m * np.dot(dZ, A_prev.T) # partial derivative of cost function w.r.t parameter W\n","    db = 1. / m * np.sum(dZ, axis=1, keepdims=True) # partial derivative of cost function w.r.t parameter b\n","    dA_prev = np.dot(W.T, dZ) # partial derivative of cost function w.r.t A (previous layer output)\n","    \n","    assert (dA_prev.shape == A_prev.shape)\n","    assert (dW.shape == W.shape)\n","    assert (db.shape == b.shape)\n","    \n","    return dA_prev, dW, db"]},{"cell_type":"markdown","source":["<a name='Linear-activation-backward'></a>\n","###Linear activation backward\n","Activations (derivative) functions are ([see activation functions](#Backward-pass)):\n","* ReLU: Used mainly in hidden layers\n","* Sigmoid: Used for Binary classification.\n","* Softmax: Used for Multi-class classification"],"metadata":{"id":"1yF9uvJPfnlI"},"id":"1yF9uvJPfnlI"},{"cell_type":"code","execution_count":null,"id":"937664c3","metadata":{"id":"937664c3"},"outputs":[],"source":["def linear_activation_backward(dA, cache, activation, AL=None, Y=None):\n","    \"\"\"\n","    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n","    \n","    Arguments:\n","    dA -- post-activation gradient for current layer l \n","    cache -- tuple of values (linear_cache, activation_cache) we store for computing backward propagation efficiently\n","    activation -- the activation to be used in this layer, stored as a text string: \"sigmoid\" or \"relu\"\n","    \n","    Returns:\n","    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n","    dW -- Gradient of the cost with respect to W (current layer l), same shape as W\n","    db -- Gradient of the cost with respect to b (current layer l), same shape as b\n","    \"\"\"\n","    linear_cache, activation_cache = cache\n","    \n","    # Derivative w.r.t the activation\n","    if activation == \"relu\":\n","        dZ = relu_backward(dA, activation_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","\n","  \n","    elif activation == \"sigmoid\":\n","        dZ = sigmoid_backward(dA, activation_cache)\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","\n","    elif activation == 'softmax':\n","        dZ = AL - Y\n","        dA_prev, dW, db = linear_backward(dZ, linear_cache)\n","    \n","    return dA_prev, dW, db"]},{"cell_type":"markdown","source":["<a name='L-layer-backward'></a>\n","###L layer backward\n","According to the half of number of parameters that indicates number of layers,<br>linear_activation_backward is used to compute dA for each layer."],"metadata":{"id":"dh_SPcn0fqmm"},"id":"dh_SPcn0fqmm"},{"cell_type":"code","execution_count":null,"id":"c2dad4ed","metadata":{"id":"c2dad4ed"},"outputs":[],"source":["def L_model_backward(AL, Y, caches, parameters, masks=None, keep_prob=1, lambd=0):\n","    \"\"\"\n","    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n","    \n","    Arguments:\n","    AL -- probability vector, output of the forward propagation (L_model_forward())\n","    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n","    caches -- list of caches containing:\n","                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n","                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n","    parameters -- weights to use in regularization term\n","    lambd -- regularization parameter\n","    masks -- python dictionary containing masks used in dropout\n","    keep_prob -- number between 0 and 1 , probability of dropout \n","\n","    Returns:\n","    grads -- A dictionary with the gradients\n","             grads[\"dA\" + str(l)] = ... \n","             grads[\"dW\" + str(l)] = ...\n","             grads[\"db\" + str(l)] = ... \n","    \"\"\"\n","    grads = {}\n","    L = len(caches) # the number of layers\n","    m = AL.shape[1]\n","    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n","    \n","    # Initializing the backpropagation\n","    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n","    \n","    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"dAL, current_cache\". Outputs: \"grads[\"dAL-1\"], grads[\"dWL\"], grads[\"dbL\"]\n","    current_cache = caches[L-1]\n","    if parameters['W' + str(L)].shape[0] != 1:\n","        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'softmax', AL=AL, Y=Y)\n","    else:  \n","        grads[\"dA\" + str(L-1)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, current_cache, 'sigmoid')\n","    \n","    # Loop from l=L-2 to l=0\n","    for l in reversed(range(L-1)):\n","        # lth layer: (RELU -> LINEAR) gradients.\n","        # Inputs: \"grads[\"dA\" + str(l + 1)], current_cache\". Outputs: \"grads[\"dA\" + str(l)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n","        current_cache = caches[l]\n","        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads[\"dA\" + str(l + 1)], current_cache, 'relu')\n","        if masks and (keep_prob != 1) and l > 0:\n","            grads[\"dA\" + str(l)] = (dA_prev_temp * masks['D' + str(l)])/keep_prob\n","        else:\n","            grads[\"dA\" + str(l)] = dA_prev_temp\n","\n","        grads[\"dW\" + str(l + 1)] = dW_temp + (lambd*parameters['W' + str(l+1)]/m)\n","        grads[\"db\" + str(l + 1)] = db_temp\n","\n","    return grads"]},{"cell_type":"markdown","id":"PaoqAsc94e99","metadata":{"id":"PaoqAsc94e99"},"source":["<a name='Learning-rate-update'></a>\n","##Learning rate update\n","Updates the value of learning rate w/r/t epoch number."]},{"cell_type":"code","execution_count":null,"id":"WhftUttN4ihv","metadata":{"id":"WhftUttN4ihv"},"outputs":[],"source":["def update_learning_rate(learning_rate0, epoch_num, decay_rate=1):\n","    \"\"\"\n","    Calculates updated the learning rate using exponential weight decay.\n","    \n","    Arguments:\n","    learning_rate0 -- Original learning rate. Scalar\n","    epoch_num -- Epoch number. Integer\n","    decay_rate -- Decay rate. Scalar\n","\n","    Returns:\n","    learning_rate -- Updated learning rate. Scalar \n","    \"\"\"\n","    \n","    learning_rate = learning_rate0 / (1 + decay_rate * epoch_num)\n","    \n","    return learning_rate"]},{"cell_type":"markdown","id":"aDoZ1o1KbNKo","metadata":{"id":"aDoZ1o1KbNKo"},"source":["<a name='Dropout'></a>\n","##Dropout\n","A regularization method."]},{"cell_type":"code","execution_count":null,"id":"BwLs4BKYbTC6","metadata":{"id":"BwLs4BKYbTC6"},"outputs":[],"source":["def dropout(A, keep_prob):\n","    \"\"\"\n","    Drops out some hidden units to regularize \n","\n","    Arguments:\n","    A -- output of a layer \n","    keep_prob -- probability of dropout\n","\n","    returns:\n","    A_new -- the A matrix with applied dropout\n","\n","    \"\"\"\n","    D = np.random.randn(*A.shape) # creating a random array \n","    D = (D < keep_prob) # set values to 0 (if larger than keep_prob) and 1 (if smaller than keep_prob)\n","    A_new = np.multiply(A, D) # shut down some values\n","    A_new = A_new / keep_prob # scale the values that haven't been shut down\n","\n","    return A_new, D"]},{"cell_type":"markdown","id":"WimNpQCdll48","metadata":{"id":"WimNpQCdll48"},"source":["<a name='Update-parameters'></a>\n","## Update parameters\n","Using the gradient of parameters, updates the parameters.<br>\n","Optimization algorithms:\n","* Gradient Descent ('gd')\n","* Gradient Descent with Momentum ('momentum')\n","* Root Mean Squared propagation ('rmsprop')\n","* Adam ('adam')"]},{"cell_type":"code","execution_count":null,"id":"203a45e8","metadata":{"id":"203a45e8"},"outputs":[],"source":["def update_parameters(parameters, grads, learning_rate, optimizer='gd', EMA=None, RMS=None, beta1=0.9, beta2=0.999, counter=None):\n","    \"\"\"\n","    Update parameters using optimizer ('defualt: 'gradiend descent')\n","    \n","    Arguments:\n","    parameters -- python dictionary containing your parameters \n","    grads -- python dictionary containing your gradients, output of L_model_backward\n","    learning_rate -- small number (alpha)\n","    optimizer -- optimization algorithm ('gd', 'momentum', 'rmsprop', 'adam')\n","    EMA -- python dictionary containing exponentially (weighted) moving averages\n","    RMS -- python dictionary containing root mean squared (weighted) moving averages\n","    beta1 -- parameter for EMA (used in 'momentum' and 'adam')\n","    beta2 -- parameter for RMS (used in 'rmsprop' and 'adam')\n","    counter -- current epoch number\n","\n","    Returns:\n","    parameters -- python dictionary containing your updated parameters \n","                  parameters[\"W\" + str(l)] = ... \n","                  parameters[\"b\" + str(l)] = ...\n","    RMS, EMA -- python dictionary containing moving averages\n","    \"\"\"\n","    \n","    L = len(parameters) // 2 # number of layers in the neural network\n","\n","    # Update rule for each parameter. Use a for loop.\n","\n","    # Gradient descent\n","    if optimizer == 'gd':\n","        for l in range(L):\n","            parameters[\"W\" + str(l+1)] -= learning_rate * grads[\"dW\" + str(l+1)]\n","            parameters[\"b\" + str(l+1)] -= learning_rate * grads[\"db\" + str(l+1)]\n","        return parameters\n","\n","    # Gradient descent with momentum\n","    elif optimizer == 'momentum':\n","        for l in range(L):\n","            # Compute exponentially (weighted) moving averages\n","            EMA['VdW' + str(l+1)] = np.multiply(beta1, EMA['VdW' + str(l+1)]) + np.multiply(1-beta1, grads['dW' + str(l+1)])\n","            EMA['Vdb' + str(l+1)] = np.multiply(beta1, EMA['Vdb' + str(l+1)]) + np.multiply(1-beta1, grads['db' + str(l+1)])\n","        \n","            # Update parameters\n","            parameters[\"W\" + str(l+1)] -= learning_rate * EMA[\"VdW\" + str(l+1)]\n","            parameters[\"b\" + str(l+1)] -= learning_rate * EMA[\"Vdb\" + str(l+1)]\n","        return (parameters, EMA)\n","    \n","    # Root mean squared propagation\n","    elif optimizer == 'rmsprop':\n","        epsilon = 1e-8\n","        for l in range(L):\n","            # Compute root mean squared\n","            RMS['SdW' + str(l+1)] = beta2*RMS['SdW' + str(l+1)] + (1-beta2)*grads['dW' + str(l+1)]**2\n","            RMS['Sdb' + str(l+1)] = beta2*RMS['Sdb' + str(l+1)] + (1-beta2)*grads['db' + str(l+1)]**2\n","\n","            # Update parameters\n","            parameters[\"W\" + str(l+1)] -= (learning_rate * grads['dW' + str(l+1)]) / (np.sqrt(RMS[\"SdW\" + str(l+1)]) + epsilon)\n","            parameters[\"b\" + str(l+1)] -= (learning_rate * grads['db' + str(l+1)]) / (np.sqrt(RMS[\"Sdb\" + str(l+1)]) + epsilon) \n","        return (parameters, RMS)\n","    \n","    # Adam optimization algorithm\n","    elif optimizer == 'adam':\n","        v_corrected = {} # Initializing first moment estimate, python dictionary\n","        s_corrected = {} # Initializing second moment estimate, python dictionary \n","        epsilon = 1e-08\n","        for l in range(L):\n","            # Compute exponentially (weighted) moving averages\n","            EMA['VdW' + str(l+1)] = np.multiply(beta1, EMA['VdW' + str(l+1)]) + np.multiply(1-beta1, grads['dW' + str(l+1)])\n","            EMA['Vdb' + str(l+1)] = np.multiply(beta1, EMA['Vdb' + str(l+1)]) + np.multiply(1-beta1, grads['db' + str(l+1)])\n","\n","            # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\"\n","            v_corrected[\"VdW\" + str(l + 1)] = EMA[\"VdW\" + str(l + 1)] / (1 - beta1**counter)\n","            v_corrected[\"Vdb\" + str(l + 1)] = EMA[\"Vdb\" + str(l + 1)] / (1 - beta1**counter)\n","\n","            # Compute root mean squared\n","            RMS['SdW' + str(l+1)] = beta2*RMS['SdW' + str(l+1)] + (1-beta2)*grads['dW' + str(l+1)]**2\n","            RMS['Sdb' + str(l+1)] = beta2*RMS['Sdb' + str(l+1)] + (1-beta2)*grads['db' + str(l+1)]**2\n","\n","            # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\"\n","            s_corrected[\"SdW\" + str(l + 1)] = RMS[\"SdW\" + str(l + 1)] / (1 - beta2**counter)\n","            s_corrected[\"Sdb\" + str(l + 1)] = RMS[\"Sdb\" + str(l + 1)] / (1 - beta2**counter)\n","\n","            # Update parameters\n","            parameters[\"W\" + str(l+1)] -= (learning_rate * v_corrected[\"VdW\" + str(l+1)]) / (np.sqrt(s_corrected[\"SdW\" + str(l+1)]) + epsilon)\n","            parameters[\"b\" + str(l+1)] -= (learning_rate * v_corrected[\"Vdb\" + str(l+1)]) / (np.sqrt(s_corrected[\"Sdb\" + str(l+1)]) + epsilon)\n","        return (parameters, EMA, RMS)\n"]},{"cell_type":"markdown","id":"D7fY754FoAKs","metadata":{"id":"D7fY754FoAKs"},"source":["<a name='Gradient-check'></a>\n","##Gradient check\n","Gradient checking used two-sided derivative to ensure that backpropagation is working correctly."]},{"cell_type":"markdown","source":["<a name='Grad-check'></a>\n","###Grad check"],"metadata":{"id":"Re-ANAk1f0Cg"},"id":"Re-ANAk1f0Cg"},{"cell_type":"code","execution_count":null,"id":"XYZHOVyioFbq","metadata":{"id":"XYZHOVyioFbq"},"outputs":[],"source":["def gradient_check(X, Y, layers_dims, epsilon=1e-7, print_iter=False):\n","    \"\"\"\n","    Checks if the gradient computing (back propagation) is working properly\n","\n","    Arguments:\n","    X --> data (inputs), numpy array of size (number of examples, num_px * num_px * 3)\n","    Y --> Ground truth labels, a numpy array of size (1, number of examples)\n","    layer_dims --> dimension of our network, python list\n","    epsilon --> a small number to use in derivative \n","    print_iter --> print iterated number each print_iter iterations, False to not print, number to print\n","\n","    Returns:\n","    diff --> difference of computed gradients, real number\n","    error --> the difference of diff and epsilon\n","    \"\"\"\n","    parameters = initialize_parameters_deep(layers_dims)\n","\n","    L = len(parameters) // 2 # number pf layers \n","\n","    AL, caches, _ = L_model_forward(X, parameters)\n","    grads =  L_model_backward(AL, Y, caches, parameters)\n","    dtheta = to_vector(grads)\n","\n","    shapes = get_shapes(parameters) # get the shapes\n","    size = 0 # size of column vector\n","\n","    for shape in shapes:\n","        size += shape[0] * shape[1]\n","\n","    dtheta_approx = np.zeros((size,1)) # initialize vector of zeros\n","    assert(dtheta.shape == dtheta_approx.shape)\n","\n","    print(size)\n","    for i in range(size):\n","        if print_iter:\n","            if i % print_iter == 0:\n","                print(i)\n","        theta = to_vector(parameters)\n","\n","        # add the epsilon\n","        theta[i] += epsilon\n","\n","        # turn to object\n","        params = from_vector(theta, shapes)\n","\n","        # forward propagation\n","        AL, caches, _ = L_model_forward(X, params)\n","\n","        # compute cost \n","        cost_pos = compute_cost(AL, Y)\n","\n","        # subtract the epsilon\n","        theta[i] -= 2 * epsilon\n","\n","        # turn to object\n","        params = from_vector(theta, shapes)\n","        assert(theta.shape == dtheta_approx.shape)\n","\n","        # forward propagation\n","        AL, caches, _ = L_model_forward(X, params)\n","\n","        # compute cost \n","        cost_neg = compute_cost(AL, Y)\n","\n","        # assign to dtheta_approx\n","        dtheta_approx[i] = (cost_pos - cost_neg) / (2 * epsilon)\n","\n","    assert(dtheta.shape == dtheta_approx.shape)\n","    # Compute the difference of calculated gradients\n","    diff = np.linalg.norm(dtheta_approx - dtheta) / (np.linalg.norm(dtheta_approx) + np.linalg.norm(dtheta))\n","    error = np.linalg.norm(epsilon - diff) # Error w.r.t epsilon\n","\n","    if diff > 2e-7:\n","        print (\"\\033[93m\" + \"There is a mistake in the backward propagation! difference = \" + str(diff) + \"\\033[0m\")\n","    else:\n","        print (\"\\033[92m\" + \"Your backward propagation works perfectly fine! difference = \" + str(diff) + \"\\033[0m\")\n","    return diff, error"]},{"cell_type":"markdown","source":["<a name='Get-shapes'></a>\n","###Get shapes"],"metadata":{"id":"1_6lF752f3z-"},"id":"1_6lF752f3z-"},{"cell_type":"code","execution_count":null,"id":"rCpsTUiKvJev","metadata":{"id":"rCpsTUiKvJev"},"outputs":[],"source":["def get_shapes(parameters):\n","    \"\"\"\n","    Gets the shapes of the paramters \n","\n","    Arguments:\n","    parameters --> an object containing all the parameters W1, b1 , ....\n","\n","    Retruns:\n","    shapes --> python list containing all parameters shapes\n","    \"\"\"\n","    shapes = [] # save shapes for later use\n","    L = len(parameters) // 2 # number of layers \n","\n","    for l in range(L):\n","        shapes.append(parameters['W' + str(l+1)].shape)\n","        shapes.append(parameters['b' + str(l+1)].shape)\n","\n","    return shapes\n"]},{"cell_type":"markdown","source":["<a name='To-vector'></a>\n","###To vector"],"metadata":{"id":"c1ssv1rYf6el"},"id":"c1ssv1rYf6el"},{"cell_type":"code","execution_count":null,"id":"jGNa07p7jUsX","metadata":{"id":"jGNa07p7jUsX"},"outputs":[],"source":["def to_vector(parameters):\n","    \"\"\"\n","    Gets all the parameters and turns them into a column vector\n","\n","    Arguments:\n","    parameters --> an object containing all the parameters W1, b1 , ....\n","\n","    Returns:\n","    theta --> the column vector containing parameters \n","    \"\"\"\n","\n","    L = len(parameters) // 2 # number of layers \n","    vectors = []\n","    for l in range(L):\n","        if 'W' + str(l+1) in parameters:\n","            # append parameters (W, b) of layer l to vectors\n","            # array.reshape(-1, 1) reshapes as a column vector\n","            vectors.append(parameters['W' + str(l+1)].reshape(-1,1)) \n","            vectors.append(parameters['b' + str(l+1)].reshape(-1,1))\n","\n","        elif 'dW' + str(l+1) in parameters:\n","            # append grads (dW, db) of layer l to vectors\n","            # array.reshape(-1, 1) reshapes as a column vector\n","            vectors.append(parameters['dW' + str(l+1)].reshape(-1,1))\n","            vectors.append(parameters['db' + str(l+1)].reshape(-1,1))\n","\n","    theta = np.concatenate(vectors, axis=0) # concatenate vectors\n","\n","    return theta"]},{"cell_type":"markdown","source":["<a name='From-vector'></a>\n","###From vector"],"metadata":{"id":"RzDAu7kigABW"},"id":"RzDAu7kigABW"},{"cell_type":"code","execution_count":null,"id":"V348G_Fol6M5","metadata":{"id":"V348G_Fol6M5"},"outputs":[],"source":["def from_vector(theta, shapes):\n","    \"\"\"\n","    Turns theta column vector to previous arrays with previous shapes\n","\n","    Arguments:\n","    theta --> column vector containing all parameters , numpy array\n","    shapes --> python list with tupled shapes in each index refering to the parameters \n","\n","    Returns:\n","    parameters --> python object containing all parameters W1, ..... \n","    \"\"\"\n","    shapes = copy.deepcopy(shapes)\n","    parameters = {}\n","    L = len(shapes) // 2 # Number of layers\n","\n","    for l in range(L):\n","        shape = shapes.pop(0) # shape of parameter W of layer l\n","        parameters['W' + str(l+1)] = theta[0:shape[0]*shape[1]].reshape(shape) # reshape as orignial shape\n","        theta = theta[shape[0]*shape[1]:] # eleminate the processed part\n","\n","        shape = shapes.pop(0) # shape of parameter b of layer l\n","        parameters['b' + str(l+1)] = theta[0:shape[0]*shape[1]].reshape(shape) # reshape as orignial shape\n","        theta = theta[shape[0]*shape[1]:] # eleminate the processed part\n","\n","    return parameters\n"]},{"cell_type":"markdown","id":"1wclfWgZq2zW","metadata":{"id":"1wclfWgZq2zW"},"source":["<a name='Mini_batch'></a>\n","##Mini_batch\n","Divides the training data into mini-batches."]},{"cell_type":"code","execution_count":null,"id":"a7gki3BRq5Km","metadata":{"id":"a7gki3BRq5Km"},"outputs":[],"source":["def random_mini_batches(X, Y, mini_batch_size = 64, seed = 0):\n","    \"\"\"\n","    Creates a list of random minibatches from (X, Y)\n","    \n","    Arguments:\n","    X -- input data, of shape (input size, number of examples)\n","    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n","    mini_batch_size -- size of the mini-batches, integer\n","    \n","    Returns:\n","    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)\n","    \"\"\"\n","    \n","    np.random.seed(seed)            # To make your \"random\" minibatches the same as ours\n","    m = X.shape[1]                  # number of training examples\n","    mini_batches = []\n","    C = Y.shape[0]\n","        \n","    # Step 1: Shuffle (X, Y)\n","    permutation = list(np.random.permutation(m))\n","    shuffled_X = X[:, permutation]\n","    shuffled_Y = Y[:, permutation].reshape((C,m))\n","\n","    # Step 2: Partition (shuffled_X, shuffled_Y). Minus the end case.\n","    num_complete_minibatches = math.floor(m/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n","    for k in range(0, num_complete_minibatches):\n","\n","        mini_batch_X = shuffled_X[:, k*mini_batch_size : (k+1)*mini_batch_size]\n","        mini_batch_Y = shuffled_Y[:, k*mini_batch_size : (k+1)*mini_batch_size]\n","\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","    \n","    # Handling the end case (last mini-batch < mini_batch_size)\n","    if m % mini_batch_size != 0:\n","        mini_batch_X = shuffled_X[:,num_complete_minibatches * mini_batch_size:]\n","        mini_batch_Y = shuffled_Y[:,num_complete_minibatches * mini_batch_size:]\n","\n","        mini_batch = (mini_batch_X, mini_batch_Y)\n","        mini_batches.append(mini_batch)\n","    \n","    return mini_batches"]},{"cell_type":"markdown","id":"P_yo3RT_mK60","metadata":{"id":"P_yo3RT_mK60"},"source":["<a name='Multi-Layer-Perceptron'></a>\n","##Multi-Layer Perceptron\n","In this function every element is put together for training."]},{"cell_type":"code","execution_count":null,"id":"22f3bb58","metadata":{"id":"22f3bb58"},"outputs":[],"source":["def L_layer_model(X, Y, layers_dims, optimizer='adam', initialization='he', mini_batch_size=64, learning_rate=0.0075,\n","                  epochs=3000, print_cost=False,keep_prob=1, lambd=0, decay=None, decay_rate=1):\n","    \"\"\"\n","    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n","    \n","    Arguments:\n","    X -- data, numpy array of shape (number of examples, num_px * num_px * 3)\n","    Y -- true \"label\" vector (containing 0 if cat, 1 if non-cat), of shape (1, number of examples)\n","    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n","    optimizer -- optimization algorithm ('gd', 'momentum', 'rmsprop', 'adam')\n","    initialization -- method of initializing parameters ('zero', 'random', 'he')\n","    learning_rate -- learning rate of the gradient descent update rule\n","    num_iterations -- number of iterations of the optimization loop\n","    print_cost -- if True, it prints the cost every 100 steps\n","    decay -- function to update learning rate\n","    decay_rate -- rate of decaying the learning rate , scalar\n","    \n","    Returns:\n","    parameters -- parameters learnt by the model. They can then be used to predict.\n","    \"\"\"\n","    # Alerting the type of optimization based on mini_batch_size\n","    message = 'Batch' if mini_batch_size == X.shape[1] else ('Stochastic' if mini_batch_size == 1 else 'mini-batch')\n","    print(message + ' Gradient Descent')\n","    \n","    np.random.seed(1)\n","    seed = 10 # Seed\n","    minibatches = random_mini_batches(X, Y, mini_batch_size, seed) # mini_batch\n","\n","    t = 0 # Moving Average counter\n","\n","    costs = [] # keep track of cost\n","    learning_rate0 = learning_rate # Saving the initial learning rate\n","    \n","    # Parameters initialization. \n","    parameters = initialize_parameters_deep(layers_dims, method=initialization)\n","    L = len(parameters) // 2\n","\n","    # Check the optimizer and initializing the requirements\n","    # Gradient descent with momentum\n","    if optimizer == 'momentum' or optimizer == 'adam':\n","        EMA = initialize_EMA(parameters) # Exponenetially (weighted) moving averages\n","      \n","    # Root mean square propagation\n","    if optimizer == 'rmsprop' or optimizer == 'adam':\n","        RMS  = initialize_RMS(parameters) # Root mean square\n","      \n","    \n","    # Loop (gradient descent)\n","    for i in range(0, epochs):\n","        seed += 1\n","        if message == 'mini-batch':\n","            minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n","\n","        for minibatch in minibatches:\n","            # Select a minibatch\n","            (minibatch_X, minibatch_Y) = minibatch\n","\n","            # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n","            AL, caches, masks = L_model_forward(minibatch_X, parameters, keep_prob=keep_prob)\n","\n","            # Compute cost.\n","            cost = compute_cost(AL, minibatch_Y, parameters=parameters, lambd=lambd)\n","\n","            # Backward propagation.\n","            grads = L_model_backward(AL, minibatch_Y, caches, parameters,masks=masks, keep_prob=keep_prob, lambd=lambd)\n","\n","            # Update parameters.\n","            if optimizer == 'gd':\n","                parameters = update_parameters(parameters, grads, learning_rate)\n","            elif optimizer == 'momentum':\n","                parameters, EMA = update_parameters(parameters, grads, learning_rate, optimizer='momentum', EMA=EMA)\n","            elif optimizer == 'rmsprop':\n","                parameters, RMS = update_parameters(parameters, grads, learning_rate, optimizer='rmsprop', RMS=RMS)\n","            elif optimizer == 'adam':\n","                t += 1\n","                parameters, EMA, RMS = update_parameters(parameters, grads, learning_rate, optimizer='adam', RMS=RMS, EMA=EMA, counter=t)\n","      \n","      # Learning rate decay\n","    if decay:\n","        learning_rate = decay(learning_rate0, i, decay_rate)\n","\n","      # Print the cost every 100 training example\n","    if print_cost and i % print_cost == 0:\n","        print (\"Cost after epoch %i: %f\" %(i, cost), end=' ')\n","        costs.append(cost)\n","    if decay:\n","        print(f'|| Learning rate: {learning_rate}')\n","            \n","    # plot the \n","    if print_cost:\n","        plt.plot(np.squeeze(costs))\n","        plt.ylabel('cost')\n","        plt.xlabel('iterations (per tens)')\n","        plt.title(\"Learning rate =\" + str(learning_rate))\n","        plt.show()\n","    \n","    return parameters"]},{"cell_type":"markdown","id":"KSOvnnyRlrV9","metadata":{"id":"KSOvnnyRlrV9"},"source":["<a name='Predict'></a>\n","## Predict"]},{"cell_type":"code","execution_count":null,"id":"2efc2b53","metadata":{"id":"2efc2b53"},"outputs":[],"source":["def predict(X, y, parameters):\n","    \"\"\"\n","    This function is used to predict the results of a  L-layer neural network.\n","    \n","    Arguments:\n","    X -- data set of examples you would like to label\n","    parameters -- parameters of the trained model\n","    \n","    Returns:\n","    p -- predictions for the given dataset X\n","    \"\"\"\n","    \n","    m = X.shape[1]\n","    n = len(parameters) // 2 # number of layers in the neural network\n","    \n","    # Forward propagation\n","    y_hat, caches, masks = L_model_forward(X, parameters)\n","  \n","    # convert probas to 0/1 predictions\n","    p = np.argmax(y_hat,axis=0).reshape(1,-1)\n","\n","    # inverse the one hot encode\n","    y = np.argmax(y, axis=0).reshape(1,-1)\n","\n","    # count the correctly predicted \n","    true_count = (p == y).sum()\n","    print(f'Correctly predicted: {true_count}')\n","    print(\"Accuracy: \"  + str(true_count / y.shape[1] * 100))\n","        \n","    return p"]},{"cell_type":"markdown","id":"ooMjM8bscSwQ","metadata":{"id":"ooMjM8bscSwQ"},"source":["<a name='Save-and-Load-model'></a>\n","##Save and Load model\n","\n"]},{"cell_type":"markdown","source":["<a name='Save-model'></a>\n","###Save model"],"metadata":{"id":"yUkqcRI8gEow"},"id":"yUkqcRI8gEow"},{"cell_type":"code","execution_count":null,"id":"zVSd7bufcVt_","metadata":{"id":"zVSd7bufcVt_"},"outputs":[],"source":["def save_model(parameters, path):\n","    \"\"\"\n","    pickes the model paramters for later use \n","\n","\n","    Arguments:\n","    paramters -- model weights, python dictionary\n","    path -- path to save model in (picke file), string\n","\n","    Returns:\n","    None\n","    \"\"\"\n","    with open(path, 'wb') as f:\n","        pickle.dump(parameters, f, protocol=pickle.HIGHEST_PROTOCOL)\n"]},{"cell_type":"markdown","source":["<a name='Load-model'></a>\n","###Load model"],"metadata":{"id":"iybyQHRagHHO"},"id":"iybyQHRagHHO"},{"cell_type":"code","execution_count":null,"id":"eRKT_xFseCrw","metadata":{"id":"eRKT_xFseCrw"},"outputs":[],"source":["def load_model(path):\n","    \"\"\"\n","    Loads model from given path \n","\n","    Arguments:\n","    path -- string, path to get model \n","\n","    Returns:\n","    parameters -- model parameters \n","    \"\"\"\n","\n","    with open(path, 'rb') as f:\n","        parameters = pickle.load(f)\n","\n","    return parameters"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"MLP_using_Numpy.ipynb","provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"}},"nbformat":4,"nbformat_minor":5}